<div align="right">
<img src="https://img.shields.io/badge/AI-ASSISTED_STUDY-3b82f6?style=for-the-badge&labelColor=1e293b&logo=bookstack&logoColor=white" alt="AI Assisted Study" />
</div>

# 02-architecture：アーキテクチャ

## はじめに

前のトピック [01-orchestration](./01-orchestration.md) では、Compose の限界から出発し、コンテナオーケストレーションがなぜ必要かを学びました

複数のマシン（ノード）にまたがるコンテナ群を自動で管理する仕組みとして、スケジューリング、管理、ネットワーキングの 3 つの柱を確認しました

また、「あるべき状態（Desired State）を宣言し、システムがその状態を維持し続ける」というオーケストレーションの核心的な考え方を導入しました

しかし、この仕組みは具体的にどのような構造で動いているのでしょうか？

複数のノードを束ねるクラスタは、どのように構成されているのでしょうか？

「あるべき状態」を誰が記憶し、誰が比較し、誰が修正するのでしょうか？

このトピックでは、オーケストレーションの<strong>アーキテクチャ（全体構造）</strong>を学びます

Kubernetes を例に、コントロールプレーンとノードの分離、各コンポーネントの役割、そして「あるべき状態」を技術的に支える仕組みを見ていきます

---

## 日常の例え

アーキテクチャの考え方を、日常の例えで見てみましょう

<strong>コントロールプレーン = 航空管制塔</strong>

空港には航空管制塔があり、どの飛行機がどの滑走路を使うか、いつ離陸・着陸するかを決定します

管制塔は自分では飛行機を飛ばしませんが、すべてのフライトの状況を把握し、指示を出し、全体の調和を保ちます

パイロット（ノード上のエージェント）は管制塔の指示に従い、自分の飛行機を操縦します

フライトスケジュールボード（etcd）には、すべてのフライトの計画が記録されています

コンテナオーケストレーションも同じ構造です

コントロールプレーンがクラスタ全体を監視・管理し、各ノードのエージェントがコンテナを実際に動かします

<strong>あるべき状態 = サーモスタット</strong>

エアコンのサーモスタットに「25 度」と設定するとします

「暑くなったらエアコンをつけて、25 度になったら止めて、また暑くなったらつけて」と細かく指示する必要はありません

「25 度」というあるべき状態を設定するだけで、サーモスタットが室温を継続的に監視し、自動で調整します

これが<strong>宣言的な管理</strong>の本質です

オーケストレーションでも、「Web サーバーを 3 つ動かしたい」と宣言するだけで、システムが自動的にその状態を維持し続けます

---

## このページで学ぶこと

このページでは、以下の概念を学びます

<strong>クラスタの構造</strong>

- <strong>コントロールプレーンとノードの分離</strong>
  - クラスタが「管理する側」と「実行する側」に分かれる構造
- <strong>API を中心とした通信</strong>
  - すべてのコンポーネントが API Server を通じて連携する仕組み

<strong>コントロールプレーンのコンポーネント</strong>

- <strong>API Server</strong>
  - クラスタの唯一の窓口
- <strong>etcd</strong>
  - クラスタの全状態を保持する記憶装置
- <strong>Scheduler</strong>
  - コンテナの配置先を決定する仕組み
- <strong>Controller Manager</strong>
  - あるべき状態を維持するコントローラ群

<strong>ノードのコンポーネント</strong>

- <strong>kubelet</strong>
  - 各ノード上でコンテナを管理するエージェント
- <strong>コンテナランタイム</strong>
  - コンテナを実際に起動・停止するソフトウェア
- <strong>kube-proxy</strong>
  - ノード上のネットワークルールを管理する仕組み

<strong>あるべき状態（Desired State）</strong>

- <strong>Reconciliation Loop（調整ループ）</strong>
  - あるべき状態と実際の状態を比較し、自動修正するメカニズム
- <strong>宣言的な管理の仕組み</strong>
  - マニフェストを通じてあるべき状態を記述する方法

---

## 目次

1. [クラスタの全体構造](#クラスタの全体構造)
2. [コントロールプレーン](#コントロールプレーン)
3. [API Server（クラスタの窓口）](#api-serverクラスタの窓口)
4. [etcd（クラスタの記憶）](#etcdクラスタの記憶)
5. [Scheduler（配置の決定）](#scheduler配置の決定)
6. [Controller Manager（あるべき状態の維持）](#controller-managerあるべき状態の維持)
7. [ノード](#ノード)
8. [kubelet（ノードのエージェント）](#kubeletノードのエージェント)
9. [コンテナランタイム](#コンテナランタイム)
10. [kube-proxy（ネットワークの管理）](#kube-proxyネットワークの管理)
11. [あるべき状態（Desired State）](#あるべき状態desired-state)
12. [次のトピックへ](#次のトピックへ)
13. [用語集](#用語集)
14. [参考資料](#参考資料)

---

## クラスタの全体構造

前のトピックで、クラスタとは複数のマシンを 1 つのまとまりとして管理するグループであると学びました

このクラスタは、大きく 2 つの役割に分かれます

<strong>コントロールプレーン（Control Plane）</strong>

クラスタ全体を管理する「頭脳」です

どのコンテナをどのノードで動かすか決め、クラスタの状態を記録し、異常を検知して修正します

コントロールプレーン自体はコンテナを動かしません

航空管制塔が飛行機を飛ばさないのと同じです

<strong>ノード（Node）</strong>

コンテナを実際に動かす「手足」です

コントロールプレーンの指示を受けて、コンテナの起動、停止、状態報告を行います

クラスタには通常、複数のノードが存在します

この 2 つの役割は、以下のように連携します

```
┌───────────────────────────────────────────────────────┐
│                  コントロールプレーン                    │
│                                                       │
│  ┌────────────┐ ┌──────┐ ┌───────────┐ ┌──────────┐  │
│  │ API Server │ │ etcd │ │ Scheduler │ │Controller│  │
│  │            │ │      │ │           │ │ Manager  │  │
│  └─────┬──────┘ └──────┘ └───────────┘ └──────────┘  │
│        │                                              │
└────────┼──────────────────────────────────────────────┘
         │
         │ すべての通信は API Server を経由
         │
    ┌────┴──────────────┬──────────────────┐
    │                   │                  │
┌───┴─────────┐  ┌──────┴──────┐  ┌───────┴─────┐
│  ノード 1   │  │  ノード 2   │  │  ノード 3   │
│             │  │             │  │             │
│  kubelet    │  │  kubelet    │  │  kubelet    │
│  runtime    │  │  runtime    │  │  runtime    │
│  kube-proxy │  │  kube-proxy │  │  kube-proxy │
└─────────────┘  └─────────────┘  └─────────────┘
```

この図で最も重要なのは、<strong>すべての通信が API Server を経由する</strong>という点です

ノードがコントロールプレーンに状態を報告するとき、Scheduler がノードに Pod を割り当てるとき、Controller Manager が状態を監視するとき、すべて API Server を通じて行います

API Server が通信の中心（ハブ）として機能することで、コンポーネント同士が直接やり取りする複雑さを避けています

### Pod とは

ここで、<strong>Pod</strong>という用語を導入します

Pod は、Kubernetes における<strong>コンテナの実行単位</strong>です

1 つの Pod には 1 つ以上のコンテナが含まれます

多くの場合、1 つの Pod に 1 つのコンテナという構成で使われます

Pod 内の複数のコンテナは、同じネットワーク空間とストレージを共有します

以降のトピックでは、「コンテナをノードに配置する」という表現の代わりに、「Pod をノードに配置する」という表現を使います

Pod は Kubernetes のスケジューリングの最小単位であり、Scheduler が配置先を決めるのは個々のコンテナではなく Pod です

---

## コントロールプレーン

コントロールプレーンは、4 つの主要なコンポーネントで構成されます

| コンポーネント     | 役割                                             |
| ------------------ | ------------------------------------------------ |
| API Server         | クラスタへのすべての操作を受け付ける窓口         |
| etcd               | クラスタの全状態を保存する分散データストア       |
| Scheduler          | Pod をどのノードで実行するかを決定する           |
| Controller Manager | あるべき状態と実際の状態を比較し、差分を修正する |

これらは互いに独立したプロセスとして動作しますが、API Server を中心に連携しています

以降のセクションで、各コンポーネントの役割を詳しく見ていきます

---

## API Server（クラスタの窓口）

<strong>API Server（kube-apiserver）</strong>は、クラスタに対するすべての操作を受け付ける<strong>唯一の窓口</strong>です

管理者がクラスタの状態を確認したいとき、新しい Pod を作成したいとき、設定を変更したいとき、すべて API Server に対してリクエストを送ります

API Server は<strong>RESTful API</strong>を提供しています

RESTful API とは、HTTP プロトコルを使ってリソースを操作する仕組みです

たとえば「Pod の一覧を取得する」「新しい Pod を作成する」「既存の Pod を削除する」といった操作を、HTTP リクエストとして表現します

### API Server の役割

API Server は、以下の処理を行います

<strong>リクエストの認証と認可</strong>

API Server は、リクエストが正当なユーザーやコンポーネントから来ているかを確認し（認証）、そのリクエストを実行する権限があるかを確認します（認可）

<strong>リクエストの検証</strong>

送られてきたリクエストの内容が正しいかを検証します

たとえば、Pod の定義に必要な項目が揃っているか、値が有効かなどを確認します

<strong>etcd への保存</strong>

検証を通過したリクエストの結果を etcd に書き込みます

API Server は、etcd と直接通信する<strong>唯一のコンポーネント</strong>です

他のコンポーネント（Scheduler、Controller Manager、kubelet）は、etcd のデータに直接アクセスせず、必ず API Server を経由します

### 誰が API Server と通信するか

| コンポーネント       | 通信の目的                                               |
| -------------------- | -------------------------------------------------------- |
| 管理者（kubectl 等） | クラスタの操作（Pod の作成、状態の確認、設定変更）       |
| Scheduler            | 未割り当ての Pod を監視し、割り当て結果を報告する        |
| Controller Manager   | リソースの状態を監視し、あるべき状態との差分を修正する   |
| kubelet              | ノードと Pod の状態を報告し、新しい Pod の仕様を取得する |

すべての通信が API Server を経由する設計には、大きな利点があります

コンポーネント間の依存関係がシンプルになり、認証・認可を API Server の 1 か所で管理できます

---

## etcd（クラスタの記憶）

<strong>etcd</strong>は、クラスタのすべての状態を保存する<strong>分散キーバリューストア</strong>です

キーバリューストアとは、「キー（名前）」と「バリュー（値）」の組み合わせでデータを保存する仕組みです

たとえば、`/registry/pods/default/web-server` というキーに、`web-server` Pod の全情報（どのノードで動いているか、どのイメージを使っているか、状態は何かなど）がバリューとして保存されます

### etcd が「クラスタの記憶」である理由

etcd には、クラスタに関するすべての情報が保存されています

- どの Pod がどのノードで動いているか
- 各 Pod のあるべき状態（何個のレプリカが必要か、どのイメージを使うかなど）
- 各ノードの状態（正常か、リソースの空きはどれくらいか）
- クラスタの設定情報

etcd は<strong>唯一の情報源（Single Source of Truth）</strong>です

クラスタの「真実」は常に etcd に記録されています

コントロールプレーンの他のコンポーネント（Scheduler、Controller Manager）は、自分自身で状態を記憶しません

必要なときに API Server を通じて etcd のデータを参照し、判断を行います

これにより、コンポーネントが再起動されても、etcd に記録された状態から処理を再開できます

### 分散と冗長性

etcd は「分散」キーバリューストアと呼ばれます

これは、etcd 自体が複数のインスタンスで動作し、データを冗長に保持できることを意味します

etcd の 1 つのインスタンスが停止しても、他のインスタンスがデータを保持しているため、クラスタの状態は失われません

複数の etcd インスタンス間でデータの一貫性を保つために、<strong>Raft</strong>と呼ばれる合意アルゴリズムが使われています

Raft は、複数のノードが「どのデータが正しいか」について合意するための仕組みです

詳細は「In Search of an Understandable Consensus Algorithm」（Ongaro & Ousterhout, 2014）で解説されていますが、ここでは「etcd は複数インスタンスで動作し、1 つが壊れてもデータが失われない」という点を理解すれば十分です

### なぜ API Server だけが etcd にアクセスするのか

etcd への直接アクセスを API Server に限定することで、以下の利点が生まれます

| 利点               | 説明                                                                                 |
| ------------------ | ------------------------------------------------------------------------------------ |
| 一貫性の保証       | すべての書き込みが API Server の検証を通るため、不正なデータが etcd に書き込まれない |
| アクセス制御の集約 | 認証・認可を API Server の 1 か所で管理できる                                        |
| 変更通知の一元化   | API Server が変更を検知し、関心のあるコンポーネントに通知できる（Watch 機能）        |

<strong>Watch 機能</strong>は、特に重要な仕組みです

Scheduler や Controller Manager は、API Server に対して「この種類のリソースに変更があったら教えてほしい」と登録します

新しい Pod が作成されたり、Pod の状態が変わったりすると、API Server がリアルタイムに通知します

これにより、各コンポーネントはデータを定期的に取得（ポーリング）する必要がなく、変更があったときだけ反応できます

---

## Scheduler（配置の決定）

<strong>Scheduler（kube-scheduler）</strong>は、<strong>まだノードに割り当てられていない Pod を見つけ、適切なノードを選んで割り当てる</strong>コンポーネントです

Scheduler は API Server の Watch 機能を使い、新しく作成された Pod を監視しています

Pod が作成されると、最初はどのノードにも割り当てられていない状態です

Scheduler はこの未割り当ての Pod を検知し、クラスタ内のノードの中から最適なノードを選びます

### Scheduler は Pod を起動しない

ここで重要なのは、<strong>Scheduler は Pod の配置先を決めるだけ</strong>ということです

Scheduler は「この Pod はノード 2 で動かすべきだ」と判断し、その結果を API Server に書き込みます

実際に Pod を起動するのは、ノード上の kubelet の役割です

航空管制塔が「この飛行機は第 3 滑走路を使え」と指示するだけで、実際に飛行機を操縦するのはパイロットであるのと同じです

### ノード選択の流れ

Scheduler がノードを選ぶ流れは、大きく 2 段階に分かれます

<strong>フィルタリング（Filtering）</strong>

まず、Pod を実行できないノードを除外します

たとえば、以下のようなノードは候補から外されます

- CPU やメモリの空きが Pod の要求量より少ないノード
- Pod が特定の条件（たとえば「SSD を持つノードのみ」）を指定している場合、条件を満たさないノード
- 障害やメンテナンス中のノード

<strong>スコアリング（Scoring）</strong>

フィルタリングを通過したノードに対して、スコアを付けます

たとえば、リソースの空きが多いノードにはより高いスコアが付きます

最もスコアの高いノードが、Pod の配置先として選ばれます

スケジューリングの詳細なアルゴリズム（リソース要求、アフィニティ、テイントなど）は、次のトピック [03-scheduling](./03-scheduling.md) で学びます

### OS スケジューラとの対比

前のシリーズでカーネル空間を学んだ方は、OS のプロセススケジューラを思い出すかもしれません

| 比較項目 | OS スケジューラ        | Kubernetes Scheduler           |
| -------- | ---------------------- | ------------------------------ |
| 対象     | プロセス / スレッド    | Pod                            |
| 配置先   | CPU コア               | ノード（マシン）               |
| 判断基準 | 優先度、タイムスライス | リソース要求、制約条件、スコア |
| 実行頻度 | ミリ秒単位             | Pod の作成時                   |

対象と規模は異なりますが、「限られたリソースに対して、何をどこで実行するかを決める」という原理は共通しています

---

## Controller Manager（あるべき状態の維持）

<strong>Controller Manager（kube-controller-manager）</strong>は、<strong>あるべき状態と実際の状態を比較し、差分があれば修正する</strong>コンポーネントです

前のトピックで導入した「あるべき状態（Desired State）」の概念を、実際に実行に移す役割を担います

### コントローラとは

Controller Manager は、複数の<strong>コントローラ</strong>を内包しています

各コントローラは、特定の種類のリソースを担当し、そのリソースのあるべき状態を維持する責任を持ちます

たとえば、<strong>ReplicaSet コントローラ</strong>は、Pod のレプリカ数を維持する責任を持ちます

「Web サーバーの Pod を 3 つ維持する」と宣言されていて、実際に動いている Pod が 2 つしかない場合、ReplicaSet コントローラが不足分の Pod を新たに作成します

逆に、Pod が 4 つ動いている場合は、1 つを削除します

### 主要なコントローラ

| コントローラ            | 担当するリソース | 役割                                                  |
| ----------------------- | ---------------- | ----------------------------------------------------- |
| ReplicaSet コントローラ | ReplicaSet       | 指定された数の Pod レプリカを維持する                 |
| Deployment コントローラ | Deployment       | ReplicaSet を管理し、ローリングアップデートを制御する |
| Node コントローラ       | Node             | ノードの状態を監視し、応答がないノードを検知する      |
| Job コントローラ        | Job              | 一度だけ実行するタスク（バッチ処理）を管理する        |

各コントローラは独立して動作し、自分の担当するリソースだけに集中します

この設計により、あるコントローラに問題が起きても、他のコントローラの動作には影響しません

### Reconciliation Loop の具体例

コントローラの動作を、<strong>ReplicaSet コントローラ</strong>の具体例で見てみましょう

管理者が「Web サーバーの Pod を 3 つ動かす」と宣言したとします

<strong>正常な状態</strong>

```
あるべき状態：Pod 3 つ
実際の状態  ：Pod 3 つ（ノード1, ノード2, ノード3）
差分        ：なし
アクション  ：何もしない
```

<strong>Pod が 1 つ停止した場合</strong>

```
あるべき状態：Pod 3 つ
実際の状態  ：Pod 2 つ（ノード1, ノード3）← ノード2 の Pod が停止
差分        ：Pod が 1 つ不足
アクション  ：新しい Pod を作成する
```

ReplicaSet コントローラは、Pod が 1 つ不足していることを検知し、API Server に対して新しい Pod の作成をリクエストします

作成された Pod は Scheduler によってノードに割り当てられ、kubelet によって起動されます

<strong>Pod が多すぎる場合</strong>

```
あるべき状態：Pod 3 つ
実際の状態  ：Pod 4 つ（ノード1, ノード2, ノード3, ノード3）
差分        ：Pod が 1 つ多い
アクション  ：余分な Pod を 1 つ削除する
```

このように、コントローラは常に「あるべき状態」と「実際の状態」を比較し、差分を自動で解消します

この継続的な比較と修正のサイクルが、<strong>Reconciliation Loop（調整ループ）</strong>です

---

## ノード

ノードは、コンテナ（Pod）を実際に動かすマシンです

各ノードには、以下の 3 つのコンポーネントが動作しています

| コンポーネント     | 役割                                                  |
| ------------------ | ----------------------------------------------------- |
| kubelet            | コントロールプレーンからの指示を受けて Pod を管理する |
| コンテナランタイム | コンテナを実際に起動・停止する                        |
| kube-proxy         | ネットワークルールを管理し、Pod への通信を制御する    |

これらのコンポーネントは、すべてのノードで同じように動作します

以降のセクションで、各コンポーネントの役割を詳しく見ていきます

---

## kubelet（ノードのエージェント）

<strong>kubelet</strong>は、各ノード上で動作する<strong>エージェント</strong>です

エージェントとは、コントロールプレーンの指示を受けて、ノード上でタスクを実行する役割を持つプロセスのことです

kubelet は、コントロールプレーンとノードを橋渡しする存在です

### kubelet の役割

<strong>Pod の仕様の取得と実行</strong>

kubelet は API Server を監視し、自分のノードに割り当てられた Pod の仕様を取得します

Pod の仕様には、どのコンテナイメージを使うか、どのポートを公開するか、どのくらいのリソースが必要かなどが記載されています

kubelet はこの仕様に基づいて、コンテナランタイムに対してコンテナの起動を指示します

<strong>ノードと Pod の状態報告</strong>

kubelet は定期的にノードの状態（CPU 使用率、メモリ使用量、ディスク空き容量など）と Pod の状態（実行中、停止、エラーなど）を API Server に報告します

この報告により、コントロールプレーンはクラスタ全体の状態を把握できます

<strong>ヘルスチェックの実行</strong>

kubelet は、Pod に設定されたヘルスチェック（Liveness Probe、Readiness Probe）を定期的に実行します

ヘルスチェックが失敗した場合、kubelet はコンテナを再起動するなどの対応を行います

### kubelet とコンテナランタイムの関係

kubelet は、コンテナを直接操作しません

代わりに、<strong>CRI（Container Runtime Interface）</strong>と呼ばれるインターフェースを通じて、コンテナランタイムに操作を依頼します

CRI とは、kubelet とコンテナランタイムの間の<strong>共通のインターフェース（取り決め）</strong>です

kubelet は CRI に準拠した命令を出すだけで、その先のコンテナランタイムが何であるかを気にする必要がありません

---

## コンテナランタイム

<strong>コンテナランタイム</strong>は、コンテナを実際に起動・停止・管理するソフトウェアです

kubelet から CRI を通じて「このコンテナを起動せよ」という指示を受け、コンテナを作成します

### CRI（Container Runtime Interface）

前のシリーズでコンテナの仕組みを学んだ方は、containerd や runc といったコンテナランタイムを思い出すかもしれません

Kubernetes では、これらのランタイムを直接呼び出すのではなく、<strong>CRI</strong>という抽象化層を介して通信します

CRI を使うことで、Kubernetes はコンテナランタイムの実装に依存しません

CRI に準拠していれば、どのランタイムでも使用できます

### コンテナ起動の流れ

kubelet がコンテナランタイムにコンテナの起動を依頼する流れは、以下のようになります

```
kubelet
  │
  │ CRI（Container Runtime Interface）
  ▼
containerd
  │
  │ OCI Runtime Specification
  ▼
runc
  │
  │ Linux カーネル
  ▼
namespace + cgroup によるコンテナの隔離
```

<strong>kubelet → containerd</strong>

kubelet は CRI を通じて containerd にコンテナの起動を依頼します

containerd は、コンテナのイメージの管理、コンテナのライフサイクル管理などを担当します

<strong>containerd → runc</strong>

containerd は、実際のコンテナ作成を runc に委譲します

runc は OCI（Open Container Initiative）ランタイム仕様に準拠した低レベルランタイムです

<strong>runc → namespace + cgroup</strong>

runc は Linux カーネルの namespace と cgroup を使って、コンテナの隔離とリソース制限を実現します

namespace はプロセスの見える範囲を分離し、cgroup はプロセスが使えるリソース（CPU、メモリなど）を制限します

前のシリーズでカーネル空間やコンテナの仕組みを学んだ方は、ここでそれらの知識がつながります

Kubernetes はこれらの既存技術の上に構築されています

---

## kube-proxy（ネットワークの管理）

<strong>kube-proxy</strong>は、各ノード上で動作し、<strong>ネットワークルールを管理する</strong>コンポーネントです

コンテナ（Pod）は作成されるたびに新しい IP アドレスが割り当てられます

Pod が再作成されると、IP アドレスが変わります

もし他の Pod がこの Pod の IP アドレスを直接使って通信していたら、Pod が再作成されるたびに通信先を更新する必要があります

これは非常に不便です

Kubernetes では、<strong>Service</strong>という抽象化を使ってこの問題を解決します

Service は、複数の Pod に対する安定したアクセス先を提供します

Pod の IP アドレスが変わっても、Service を通じてアクセスすれば、常に正しい Pod に到達できます

kube-proxy は、このService が機能するために必要なネットワークルールを各ノード上に設定します

具体的には、Service 宛ての通信を適切な Pod に転送するルールを管理します

Service とサービスディスカバリの詳細は、[04-service-discovery](./04-service-discovery.md) で学びます

---

## あるべき状態（Desired State）

ここまで、コントロールプレーンとノードの各コンポーネントの役割を見てきました

このセクションでは、それらを貫く<strong>最も重要な概念</strong>である「あるべき状態（Desired State）」を掘り下げます

### 宣言的と命令的

前のトピックで簡単に触れた「宣言的」と「命令的」の違いを、もう少し詳しく見てみましょう

<strong>命令的（Imperative）</strong>

具体的な操作手順を 1 つずつ指示する方法です

```
1. ノード 1 に Web サーバーのコンテナを起動する
2. ノード 2 に Web サーバーのコンテナを起動する
3. ノード 3 に Web サーバーのコンテナを起動する
4. もしノード 1 のコンテナが停止したら、ノード 1 で再起動する
5. もしノード 1 自体が故障したら、ノード 4 でコンテナを起動する
```

この方法では、あらゆる状況を事前に想定して手順を書く必要があります

想定外の状況が起きると、対応できません

<strong>宣言的（Declarative）</strong>

最終的にあるべき状態だけを宣言する方法です

```
Web サーバーの Pod を常に 3 つ動かす
```

具体的にどのノードで動かすか、障害時にどう対応するかは、システム（Kubernetes）が自動で判断します

この違いは、サーモスタットの例えで説明した通りです

宣言的な方法では、管理者は「何をしたいか」を伝えるだけで、「どうやるか」はシステムに任せます

### Reconciliation Loop（調整ループ）

あるべき状態を実現するために、Kubernetes は<strong>Reconciliation Loop（調整ループ）</strong>というメカニズムを使います

これは、以下の 3 つのステップを<strong>継続的に繰り返す</strong>サイクルです

```
         ┌───────────────────────────────────────────┐
         │                                           │
         ▼                                           │
    ┌─────────┐                                      │
    │ 観察    │ 実際の状態を確認する                   │
    │(Observe)│                                      │
    └────┬────┘                                      │
         │                                           │
         ▼                                           │
    ┌─────────┐                                      │
    │ 比較    │ あるべき状態と実際の状態を比較する      │
    │ (Diff)  │                                      │
    └────┬────┘                                      │
         │                                           │
         ▼                                           │
    ┌─────────┐                                      │
    │ 調整    │ 差分があれば、あるべき状態に            │
    │  (Act)  │ 近づくための操作を実行する             │
    └────┬────┘                                      │
         │                                           │
         └───────────────────────────────────────────┘

              Reconciliation Loop（調整ループ）
```

<strong>観察（Observe）</strong>

コントローラが API Server を通じて、リソースの実際の状態を確認します

たとえば、「Web サーバーの Pod がいくつ動いているか」を確認します

<strong>比較（Diff）</strong>

etcd に記録されているあるべき状態と、実際の状態を比較します

「あるべき状態は Pod 3 つだが、実際に動いているのは 2 つ」という差分を検知します

<strong>調整（Act）</strong>

差分を解消するために必要な操作を実行します

「Pod が 1 つ不足しているから、新しい Pod を作成する」という操作を API Server に依頼します

この 3 ステップが完了すると、再び「観察」に戻り、サイクルが繰り返されます

変更があっても、障害が起きても、この調整ループが継続的に動くことで、システムは常にあるべき状態に向かって収束します

### マニフェストによる宣言

あるべき状態は、<strong>マニフェスト</strong>と呼ばれる定義ファイルで記述します

マニフェストは YAML 形式で書かれることが一般的です

以下は、「Web サーバーの Pod を 3 つ動かす」というあるべき状態を記述したマニフェストの例です

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-server
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
        - name: web
          image: nginx
```

このマニフェストは「このような状態であるべき」という宣言です

マニフェストの書き方そのものはこのリポジトリの目的ではありませんが、重要な点が 1 つあります

`replicas: 3` は、「Pod を 3 つ起動しろ」という命令ではありません

「Pod が 3 つ動いている状態を維持せよ」という<strong>宣言</strong>です

Pod が 1 つ停止すれば、Reconciliation Loop が差分を検知し、自動で新しい Pod を作成します

Pod が 4 つに増えれば、1 つを削除します

管理者がすべきことは、「あるべき状態」をマニフェストに書いて API Server に送るだけです

### あるべき状態と各コンポーネントの関係

ここまで学んだコンポーネントが、あるべき状態の維持にどう関わるかをまとめます

| コンポーネント     | あるべき状態との関わり                                               |
| ------------------ | -------------------------------------------------------------------- |
| etcd               | あるべき状態と実際の状態を保存する                                   |
| API Server         | あるべき状態の登録・変更を受け付け、各コンポーネントに変更を通知する |
| Controller Manager | あるべき状態と実際の状態を比較し、差分を修正するアクションを起こす   |
| Scheduler          | Controller Manager が作成した Pod を、適切なノードに割り当てる       |
| kubelet            | 割り当てられた Pod を実際に起動し、状態を報告する                    |

この協調動作によって、管理者が宣言した「あるべき状態」が実現され、維持されます

### このリポジトリ全体との関係

「あるべき状態」の概念は、以降のすべてのトピックに登場します

| トピック                                                    | あるべき状態との関係                                                     |
| ----------------------------------------------------------- | ------------------------------------------------------------------------ |
| [03-scheduling](./03-scheduling.md)                         | あるべき状態を実現するために、Pod をどのノードに配置するかを決める       |
| [04-service-discovery](./04-service-discovery.md)           | Pod が再作成されて IP が変わっても、Service が安定したアクセスを維持する |
| [05-self-healing](./05-self-healing.md)                     | 実際の状態があるべき状態から逸脱したとき、自動で修復する仕組み           |
| [06-scaling](./06-scaling.md)                               | 負荷に応じてあるべき状態そのものを動的に変更する仕組み                   |
| [07-declarative-management](./07-declarative-management.md) | あるべき状態をマニフェストとして管理する方法と全体の統合                 |

---

## 次のトピックへ

このトピックでは、以下のことを学びました

- クラスタは「管理する側（コントロールプレーン）」と「実行する側（ノード）」に分かれている
- コントロールプレーンの 4 つのコンポーネント（API Server、etcd、Scheduler、Controller Manager）がそれぞれ異なる責務を持ち、API Server を中心に連携している
- etcd がクラスタの唯一の情報源であり、すべての状態を保存している
- ノードでは kubelet がコントロールプレーンの指示を受け、CRI を通じてコンテナランタイムにコンテナの起動を依頼する
- 「あるべき状態（Desired State）」を宣言し、Reconciliation Loop で継続的に維持するのがオーケストレーションの核心的な仕組みである

アーキテクチャの全体像が見えたところで、いくつかの疑問が生まれます

Scheduler は Pod の配置先をどのようなアルゴリズムで選択するのでしょうか？

Pod が必要とする CPU やメモリの量は、スケジューリングにどう影響するのでしょうか？

「この Pod は SSD を持つノードでしか動かさない」といった制約はどう実現されるのでしょうか？

次のトピック [03-scheduling](./03-scheduling.md) では、<strong>スケジューリング</strong>の仕組みを詳しく学びます

---

## 用語集

| 用語                                          | 説明                                                                                                                      |
| --------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------- |
| アーキテクチャ（Architecture）                | システムの全体構造。コンポーネントの構成と、それらの関係を指す                                                            |
| クラスタ（Cluster）                           | 複数のマシン（ノード）を 1 つのまとまりとして管理するグループ                                                             |
| コントロールプレーン（Control Plane）         | クラスタ全体を管理する中枢。API Server、etcd、Scheduler、Controller Manager で構成される                                  |
| ノード（Node）                                | クラスタを構成する個々のマシン。Pod を実際に実行する                                                                      |
| Pod                                           | Kubernetes におけるコンテナの実行単位。1 つ以上のコンテナを含み、スケジューリングの最小単位となる                         |
| API Server（kube-apiserver）                  | クラスタに対するすべての操作を受け付ける唯一の窓口。RESTful API を提供する                                                |
| RESTful API                                   | HTTP プロトコルを使ってリソースを操作する仕組み。取得、作成、更新、削除などの操作を HTTP メソッドで表現する               |
| etcd                                          | クラスタのすべての状態を保存する分散キーバリューストア。唯一の情報源（Single Source of Truth）として機能する              |
| キーバリューストア（Key-Value Store）         | キー（名前）とバリュー（値）の組み合わせでデータを保存するデータストア                                                    |
| 唯一の情報源（Single Source of Truth）        | システムの正しい状態が記録されている、唯一の場所                                                                          |
| Raft                                          | 複数のノード間でデータの一貫性を保つための合意アルゴリズム。etcd が内部で使用している                                     |
| Watch 機能                                    | API Server がリソースの変更をリアルタイムに通知する仕組み。Scheduler や Controller Manager がこれを利用して変更を検知する |
| Scheduler（kube-scheduler）                   | まだノードに割り当てられていない Pod を見つけ、適切なノードを選んで割り当てるコンポーネント                               |
| フィルタリング（Filtering）                   | Scheduler がノード選択時に、Pod を実行できないノードを候補から除外するステップ                                            |
| スコアリング（Scoring）                       | Scheduler がフィルタリング後のノード候補にスコアを付け、最適なノードを選ぶステップ                                        |
| Controller Manager（kube-controller-manager） | 複数のコントローラを内包し、あるべき状態と実際の状態の差分を自動修正するコンポーネント                                    |
| コントローラ（Controller）                    | 特定のリソースのあるべき状態を維持する責任を持つ制御ループ                                                                |
| ReplicaSet                                    | 指定された数の Pod レプリカを維持するリソース                                                                             |
| Deployment                                    | ReplicaSet を管理し、ローリングアップデートなどのデプロイ戦略を制御するリソース                                           |
| kubelet                                       | 各ノード上で動作するエージェント。コントロールプレーンの指示を受けて Pod を管理する                                       |
| CRI（Container Runtime Interface）            | kubelet とコンテナランタイムの間の標準インターフェース。ランタイムの実装に依存しない設計を可能にする                      |
| containerd                                    | CRI に準拠した高レベルコンテナランタイム。コンテナイメージの管理やコンテナのライフサイクル管理を担当する                  |
| runc                                          | OCI ランタイム仕様に準拠した低レベルコンテナランタイム。Linux カーネルの namespace と cgroup を使ってコンテナを作成する   |
| OCI（Open Container Initiative）              | コンテナのランタイム仕様とイメージ仕様を標準化する団体                                                                    |
| namespace                                     | Linux カーネルの機能。プロセスの見える範囲（PID、ネットワーク、ファイルシステムなど）を分離する                           |
| cgroup（Control Group）                       | Linux カーネルの機能。プロセスが使用できるリソース（CPU、メモリなど）を制限する                                           |
| kube-proxy                                    | 各ノード上でネットワークルールを管理し、Service への通信を適切な Pod に転送するコンポーネント                             |
| Service                                       | 複数の Pod に対する安定したアクセス先を提供する抽象化。Pod の IP が変わっても、Service を通じて到達できる                 |
| あるべき状態（Desired State）                 | システムが維持すべき目標の状態。マニフェストを通じて管理者が宣言する                                                      |
| 実際の状態（Current State / Actual State）    | システムのその時点での実際の状態。あるべき状態と比較される                                                                |
| 宣言的（Declarative）                         | 最終的にあるべき状態を宣言し、システムにその実現を任せる方法                                                              |
| 命令的（Imperative）                          | 具体的な操作手順を 1 つずつ指示する方法                                                                                   |
| Reconciliation Loop（調整ループ）             | 観察→比較→調整のサイクルを継続的に繰り返し、実際の状態をあるべき状態に収束させるメカニズム                                |
| マニフェスト（Manifest）                      | あるべき状態を記述した定義ファイル。YAML 形式で書かれることが一般的                                                       |

---

## 参考資料

このページの内容は、以下のソースに基づいています

<strong>Kubernetes アーキテクチャ</strong>

- [Kubernetes Components](https://kubernetes.io/docs/concepts/overview/components/)
  - コントロールプレーンとノードの各コンポーネントの公式説明

- [The Kubernetes API](https://kubernetes.io/docs/concepts/overview/kubernetes-api/)
  - API Server と RESTful API の仕様

<strong>etcd と分散合意</strong>

- [etcd Documentation](https://etcd.io/docs/)
  - 分散キーバリューストア etcd の公式ドキュメント

- "In Search of an Understandable Consensus Algorithm" (Ongaro & Ousterhout, 2014)
  - Raft 合意アルゴリズムの論文

<strong>コンテナランタイム</strong>

- [Container Runtime Interface (CRI)](https://kubernetes.io/docs/concepts/architecture/cri/)
  - kubelet とコンテナランタイム間のインターフェース仕様

- [containerd Documentation](https://containerd.io/docs/)
  - containerd の公式ドキュメント

- [OCI Runtime Specification](https://github.com/opencontainers/runtime-spec)
  - コンテナランタイムの標準仕様

<strong>起源</strong>

- "Large-scale cluster management at Google with Borg" (Verma et al., EuroSys 2015)
  - Google の大規模クラスタ管理システム Borg の設計と運用

- "Borg, Omega, and Kubernetes" (Burns et al., ACM Queue 2016)
  - Borg から Kubernetes への設計思想の変遷と教訓
