<div align="right">
<img src="https://img.shields.io/badge/AI-ASSISTED_STUDY-3b82f6?style=for-the-badge&labelColor=1e293b&logo=bookstack&logoColor=white" alt="AI Assisted Study" />
</div>

# 06-scaling：スケーリング

## はじめに

前のトピック [05-self-healing](./05-self-healing.md) では、障害を自動で検知し、あるべき状態に戻すセルフヒーリングの仕組みを学びました

Reconciliation Loop がコンテナの異常終了、Pod の消失、ノードのダウンを検知し、コンテナの再起動、Pod の再作成、Pod の退避を自動で行うことを確認しました

セルフヒーリングにより、あるべき状態を「維持する」仕組みが揃いました

しかし、ここで新たな疑問が生まれます

あるべき状態が「Pod を 3 つ動かす」のとき、セルフヒーリングは Pod を 3 つに維持します

しかし、アクセスが急増して 3 つでは処理しきれなくなったらどうなるでしょうか？

あるべき状態そのもの、つまり Pod の数を「3 つ」から「10 個」に動的に変更する仕組みはあるのでしょうか？

負荷が下がったとき、再び Pod の数を減らすことはできるのでしょうか？

このトピックでは、<strong>スケーリング</strong>の仕組みを学びます

負荷に応じてあるべき状態を動的に変更し、Pod の数を自動で増減させるメカニズムを見ていきます

---

## 日常の例え

スケーリングの考え方を、日常の例えで見てみましょう

<strong>スケーリング = レストランのスタッフ管理</strong>

レストランを経営しているとします

平日の昼間は客が少なく、ウェイターは 2 人で足ります

しかし、週末のディナータイムには客が殺到し、2 人では対応しきれません

このとき、ウェイターを 5 人に増やせば、客を待たせずにサービスを提供できます

客が減ったら、再び 2 人に戻せばコストを抑えられます

<strong>手動スケーリング = 店長が毎回判断する</strong>

店長が「今日は混みそうだからウェイターを増やそう」「空いてきたから減らそう」と毎回判断するのが手動スケーリングです

店長は 24 時間お店にいるわけではないので、急な混雑には対応できません

<strong>自動スケーリング = 混雑センサーによる自動調整</strong>

お店に混雑センサーを設置し、「テーブル稼働率が 70% を超えたらウェイターを増やす」「50% を下回ったら減らす」というルールを設定します

センサーが混雑状況を常に監視し、ルールに従って自動でスタッフを増減させます

Kubernetes のスケーリングも同じ仕組みです

CPU 使用率やリクエスト数などのメトリクスを監視し、あらかじめ設定した目標値に基づいて Pod の数を自動で増減させます

<strong>水平スケーリングと垂直スケーリング</strong>

客が増えたとき、対応方法は 2 つあります

<strong>水平スケーリング</strong>は、ウェイターの人数を増やす方法です

<strong>垂直スケーリング</strong>は、1 人のウェイターの能力を高める方法です（たとえば、ベテランを投入する）

Kubernetes は水平スケーリング、つまり Pod の数を増減させる方法を中心に設計されています

---

## このページで学ぶこと

このページでは、以下の概念を学びます

<strong>スケーリングの基本</strong>

- <strong>水平スケーリングと垂直スケーリング</strong>
  - Pod の数を増減させる方法と、Pod のリソース割り当てを変更する方法
- <strong>手動スケーリング</strong>
  - レプリカ数を手動で変更する仕組みと、その限界

<strong>自動スケーリング（HPA）</strong>

- <strong>Horizontal Pod Autoscaler（HPA）</strong>
  - メトリクスに基づいてレプリカ数を自動で調整するコントローラ
- <strong>メトリクスに基づく判断</strong>
  - CPU 使用率やカスタムメトリクスを使った自動スケールの仕組み
- <strong>HPA のアルゴリズム</strong>
  - 必要なレプリカ数を計算する具体的な計算式

<strong>スケーリングの安定性</strong>

- <strong>スケールアップとスケールダウンの制御</strong>
  - 急激な増減を防ぎ、安定したスケーリングを実現する仕組み

---

## 目次

1. [スケーリングとは](#スケーリングとは)
2. [手動スケーリング](#手動スケーリング)
3. [HPA（Horizontal Pod Autoscaler）](#hpahorizontal-pod-autoscaler)
4. [メトリクスに基づく判断](#メトリクスに基づく判断)
5. [HPA のアルゴリズム](#hpa-のアルゴリズム)
6. [スケールアウトとスケールイン](#スケールアウトとスケールイン)
7. [垂直スケーリング（対比として）](#垂直スケーリング対比として)
8. [スケーリングの全体像](#スケーリングの全体像)
9. [次のトピックへ](#次のトピックへ)
10. [用語集](#用語集)
11. [参考資料](#参考資料)

---

## スケーリングとは

### Compose からの振り返り

前のシリーズでコンテナ管理を学んだ方は、Compose で `--scale` オプションや `replicas` を使ってコンテナの数を増やせたことを思い出すかもしれません

しかし、Compose のスケーリングには制約がありました

| 制約                       | 説明                                                             |
| -------------------------- | ---------------------------------------------------------------- |
| 手動操作が必要             | レプリカ数の増減は管理者が手動で行う                             |
| 単一マシンに限定           | 1 台のマシン上でしかコンテナを増やせない                         |
| 負荷に応じた自動調整がない | CPU 使用率やリクエスト数に基づいた自動スケーリングの仕組みがない |

Kubernetes のスケーリングは、これらの制約を解決します

### 2 つの方向

スケーリングには 2 つの方向があります

<strong>水平スケーリング（Horizontal Scaling）</strong>

Pod の数を増減させる方法です

「Pod を 3 つから 5 つに増やす」「5 つから 3 つに減らす」のように、同じ構成の Pod を横に並べて数を変えます

水平に増やすことを<strong>スケールアウト</strong>、水平に減らすことを<strong>スケールイン</strong>と呼びます

<strong>垂直スケーリング（Vertical Scaling）</strong>

Pod に割り当てるリソース（CPU やメモリ）を変更する方法です

「CPU の割り当てを 0.5 コアから 1 コアに増やす」のように、1 つの Pod の能力を強化します

垂直に強化することを<strong>スケールアップ</strong>、垂直に弱化することを<strong>スケールダウン</strong>と呼びます

### なぜ水平スケーリングが中心か

Kubernetes は水平スケーリングを中心に設計されています

水平スケーリングには、垂直スケーリングにはない利点があります

| 観点           | 水平スケーリング                     | 垂直スケーリング                              |
| -------------- | ------------------------------------ | --------------------------------------------- |
| 上限           | ノードを追加すれば際限なくスケール可 | 1 台のノードのリソース上限に制約される        |
| 可用性         | Pod が複数あるため、1 つの障害に強い | Pod が 1 つなら、障害時にサービスが停止する   |
| 再起動の必要性 | 新しい Pod を追加するだけ            | リソース変更に Pod の再起動が必要な場合がある |

水平スケーリングは「同じ Pod を増やすだけ」というシンプルさがあり、あるべき状態のレプリカ数を変更するだけで実現できます

---

## 手動スケーリング

### レプリカ数の変更

最も基本的なスケーリングは、<strong>レプリカ数を手動で変更する</strong>方法です

[02-architecture](./02-architecture.md) で学んだ Reconciliation Loop を思い出してください

あるべき状態として「Pod を 3 つ動かす」と宣言されているとき、この数を「5 つ」に変更すれば、Reconciliation Loop が差分を検知し、新しい Pod を 2 つ作成します

```
変更前：あるべき状態 = Pod 3 つ、実際の状態 = Pod 3 つ → 差分なし
  │
  ▼
レプリカ数を 5 に変更
  │
  ▼
変更後：あるべき状態 = Pod 5 つ、実際の状態 = Pod 3 つ → Pod が 2 つ不足
  │
  ▼
Reconciliation Loop が Pod を 2 つ作成
  │
  ▼
実際の状態 = Pod 5 つ → あるべき状態と一致
```

新しく作成された Pod は、[03-scheduling](./03-scheduling.md) で学んだ Scheduler によって適切なノードに配置されます

[04-service-discovery](./04-service-discovery.md) で学んだ EndpointSlice に新しい Pod が追加され、Service を通じてトラフィックが転送されます

### 手動スケーリングの限界

手動スケーリングは仕組みとしてはシンプルですが、実用上の問題があります

| 問題                | 説明                                                                       |
| ------------------- | -------------------------------------------------------------------------- |
| 判断の遅れ          | 負荷の急増に管理者が気づくまでに時間がかかる                               |
| 24 時間の監視が必要 | 深夜や休日にも負荷の変動は起きるが、管理者は常に監視できない               |
| 予測の困難さ        | 将来の負荷を正確に予測してレプリカ数を決めることは難しい                   |
| スケールインの遅れ  | 負荷が下がった後も、管理者がレプリカ数を減らさなければリソースが無駄になる |

これらの問題を解決するために、メトリクスに基づいてレプリカ数を自動で調整する仕組みが必要です

---

## HPA（Horizontal Pod Autoscaler）

### HPA とは

<strong>Horizontal Pod Autoscaler（HPA）</strong>は、<strong>メトリクスに基づいてレプリカ数を自動で調整するコントローラ</strong>です

HPA は、Pod の CPU 使用率やメモリ使用率などのメトリクスを定期的に確認し、あらかじめ設定した目標値との差に基づいて、レプリカ数を自動で増減させます

### セルフヒーリングとの関係

[05-self-healing](./05-self-healing.md) で学んだセルフヒーリングは、<strong>あるべき状態を「維持する」</strong>仕組みでした

「Pod を 3 つ動かす」という宣言に対して、Pod が 1 つ停止すれば自動で 3 つに戻します

HPA は、<strong>あるべき状態そのものを「変更する」</strong>仕組みです

「Pod を 3 つ動かす」という宣言を、負荷に応じて「Pod を 5 つ動かす」に自動で書き換えます

```
セルフヒーリング（05 で学んだ仕組み）
  あるべき状態：Pod 3 つ → Pod が 1 つ停止 → 3 つに戻す（状態を維持）

HPA（このトピックで学ぶ仕組み）
  あるべき状態：Pod 3 つ → 負荷が増加 → 5 つに変更（状態を動的に変更）
```

セルフヒーリングと HPA は対立するものではなく、補完し合う関係です

HPA があるべき状態を「5 つ」に変更した後、Pod が 1 つ停止すれば、セルフヒーリングが「5 つ」に戻します

### HPA の動作の流れ

HPA は、以下の流れで動作します

```
1. HPA がメトリクスを定期的に確認する（デフォルトでは 15 秒ごと）
   │
   ▼
2. 現在のメトリクス値と目標値を比較する
   │
   ▼
3. 必要なレプリカ数を計算する
   │
   ▼
4. Deployment のレプリカ数を変更する
   │
   ▼
5. Reconciliation Loop が新しいレプリカ数に合わせて Pod を増減する
```

HPA 自身は Pod を直接作成しません

HPA は Deployment（または ReplicaSet）のレプリカ数を変更するだけであり、実際の Pod の増減は Reconciliation Loop が行います

これは、ここまで学んできた Kubernetes の一貫した設計です

「あるべき状態を変更する」のと「あるべき状態に合わせて調整する」のは、別の役割として分離されています

---

## メトリクスに基づく判断

HPA は、何を基準にレプリカ数を決めるのでしょうか

その基準となるのが、<strong>メトリクス</strong>です

### メトリクスとは

<strong>メトリクス</strong>とは、システムの状態を数値化した<strong>測定値</strong>です

CPU 使用率、メモリ使用量、1 秒あたりのリクエスト数などが代表的なメトリクスです

HPA は、これらのメトリクスを使って「今、Pod に負荷がかかっているか」を判断します

### CPU 使用率を例にした判断

最も一般的なメトリクスは <strong>CPU 使用率</strong>です

[03-scheduling](./03-scheduling.md) で、Pod にはリソース要求（requests）を設定できることを学びました

CPU のリソース要求が 500m（0.5 コア）の Pod が、実際に 400m を使用している場合、CPU 使用率は 80% です

HPA に「CPU 使用率の目標を 50% にする」と設定すれば、CPU 使用率が 50% を超えたときに Pod を増やし、50% を下回ったときに Pod を減らします

### Metrics API

HPA がメトリクスを取得するには、メトリクスを収集して提供する仕組みが必要です

Kubernetes はこの仕組みを <strong>Metrics API</strong>として定義しています

Metrics API の実装として一般的に使われるのが <strong>metrics-server</strong>です

metrics-server は各ノードの kubelet からリソース使用量を収集し、Metrics API を通じて HPA に提供します

```
kubelet（ノード上） → メトリクスを報告 → metrics-server → Metrics API → HPA
```

### メトリクスの種類

HPA が使用できるメトリクスの種類は主に以下の通りです

| 種類               | 説明                                             | 例                             |
| ------------------ | ------------------------------------------------ | ------------------------------ |
| リソースメトリクス | Pod の CPU やメモリの使用率                      | CPU 使用率 50%                 |
| カスタムメトリクス | アプリケーション固有のメトリクス                 | 1 秒あたりのリクエスト数       |
| 外部メトリクス     | クラスタ外部の監視システムから取得するメトリクス | メッセージキューのメッセージ数 |

リソースメトリクスは Metrics API（metrics-server）から取得します

カスタムメトリクスと外部メトリクスは、別途メトリクス収集の仕組みを導入する必要があります

---

## HPA のアルゴリズム

HPA は、具体的にどのようにレプリカ数を計算するのでしょうか

### 計算式

HPA は以下の計算式で必要なレプリカ数を算出します

```
必要レプリカ数 = ceil( 現在のレプリカ数 × ( 現在のメトリクス値 / 目標メトリクス値 ) )
```

`ceil` は切り上げです

この計算式は「現在のメトリクス値が目標値からどれだけ離れているか」の比率でレプリカ数を調整するという考え方です

### 具体例

<strong>スケールアウトの例（Pod を増やす）</strong>

3 つの Pod が動いており、CPU 使用率の目標が 50% に設定されているとします

現在の平均 CPU 使用率が 90% の場合、計算は以下になります

```
必要レプリカ数 = ceil( 3 × (90 / 50) )
              = ceil( 3 × 1.8 )
              = ceil( 5.4 )
              = 6
```

HPA はレプリカ数を 3 から 6 に増やします

Pod が 6 つに増えれば、負荷が分散され、平均 CPU 使用率は目標の 50% 付近に下がります

<strong>スケールインの例（Pod を減らす）</strong>

6 つの Pod が動いており、現在の平均 CPU 使用率が 25% の場合、計算は以下になります

```
必要レプリカ数 = ceil( 6 × (25 / 50) )
              = ceil( 6 × 0.5 )
              = ceil( 3.0 )
              = 3
```

HPA はレプリカ数を 6 から 3 に減らします

### 許容範囲（tolerance）

メトリクスの値がわずかに変動するたびにレプリカ数が変わると、Pod が頻繁に作成・削除されてしまいます

これを防ぐために、HPA にはデフォルトで 0.1（10%）の<strong>許容範囲</strong>が設定されています

計算結果の比率（現在のメトリクス値 / 目標メトリクス値）が 1.0 の前後 10% 以内（0.9〜1.1）であれば、HPA はスケーリングを行いません

### HPA のマニフェスト例

HPA の仕組みを理解するために、マニフェストの例を見てみましょう

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-deployment
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50
```

このマニフェストは以下を宣言しています

- `scaleTargetRef`：スケーリング対象は `web-deployment` という Deployment
- `minReplicas`：最小レプリカ数は 2（これ以下には減らさない）
- `maxReplicas`：最大レプリカ数は 10（これ以上は増やさない）
- `metrics`：CPU 使用率の平均が 50% になるようにレプリカ数を調整する

`minReplicas` と `maxReplicas` によって、スケーリングの範囲に上限と下限が設けられています

負荷がゼロになっても Pod を 0 にはしないし、負荷が急増しても無制限には増やしません

---

## スケールアウトとスケールイン

HPA のスケーリングは、増やす（スケールアウト）ときと減らす（スケールイン）ときで異なる性質を持ちます

### スケールアウト（Pod を増やす）

負荷が急増した場合、迅速に Pod を増やす必要があります

HPA はスケールアウトに対して<strong>即応的</strong>に動作します

メトリクスが目標値を超えたことを検知すると、すぐにレプリカ数を増やします

これにより、アクセスの急増に対して素早く対応できます

### スケールイン（Pod を減らす）

一方、負荷が下がった場合の Pod の削減には慎重さが必要です

たとえば、一時的に負荷が下がっただけなのにすぐに Pod を減らしてしまうと、再び負荷が上がったときに Pod が不足します

再び Pod を増やし、また負荷が下がって減らし、という繰り返しが起きます

この不安定な状態を<strong>フラッピング</strong>と呼びます

### 安定化ウィンドウ

フラッピングを防ぐために、HPA には<strong>安定化ウィンドウ（stabilization window）</strong>が設定されています

安定化ウィンドウは、スケーリングの決定を行う際に、過去の一定期間に計算された推奨レプリカ数を参照し、その中から最適な値を選ぶ仕組みです

<strong>スケールインの安定化ウィンドウ</strong>

デフォルトでは 300 秒（5 分）に設定されています

過去 5 分間に計算された推奨レプリカ数の中から最大値を採用します

一時的に負荷が下がっても、5 分間は Pod を減らさず、安定した負荷低下を確認してから削減します

<strong>スケールアウトの安定化ウィンドウ</strong>

デフォルトでは 0 秒に設定されています

負荷の増加は即座に対応する必要があるため、遅延なくスケールアウトします

### スケーリングの比較

| 方向           | 安定化ウィンドウ（デフォルト） | 性質   | 理由                                                                       |
| -------------- | ------------------------------ | ------ | -------------------------------------------------------------------------- |
| スケールアウト | 0 秒（即座）                   | 即応的 | 負荷増加は迅速に対応しないとサービスに影響する                             |
| スケールイン   | 300 秒（5 分）                 | 慎重   | 一時的な負荷低下でのフラッピング（レプリカ数の頻繁な増減の繰り返し）を防ぐ |

この非対称な設計により、「増やすときは素早く、減らすときは慎重に」という安定したスケーリングが実現されます

---

## 垂直スケーリング（対比として）

ここまで学んできた水平スケーリングに対して、垂直スケーリングについても触れておきます

### 垂直スケーリングとは

<strong>垂直スケーリング</strong>は、Pod の数を変えるのではなく、<strong>Pod に割り当てるリソース（CPU やメモリ）を変更する</strong>方法です

「CPU を 0.5 コアから 1 コアに増やす」「メモリを 256 MiB から 512 MiB に増やす」のように、1 つの Pod の処理能力を強化します

Kubernetes では <strong>VPA（Vertical Pod Autoscaler）</strong>が、この垂直スケーリングを自動化する仕組みとして存在します

### 水平スケーリングとの比較

| 観点           | 水平スケーリング（HPA）            | 垂直スケーリング（VPA）                  |
| -------------- | ---------------------------------- | ---------------------------------------- |
| 変更対象       | Pod の数                           | Pod のリソース割り当て                   |
| スケール上限   | ノード追加で拡張可能               | ノードのリソース上限に制約               |
| 可用性への影響 | Pod が分散するため影響が小さい     | リソース変更時に再起動が必要な場合がある |
| 適用範囲       | ステートレスなアプリケーション向き | ステートフルなアプリケーション向き       |

Kubernetes は水平スケーリングを主軸に設計されています

HPA は Kubernetes のコア機能として組み込まれていますが、VPA は追加のコンポーネントとして導入する必要があります

水平スケーリングがあるべき状態のレプリカ数を変更するのに対し、垂直スケーリングはあるべき状態のリソース要求を変更するという違いがあります

---

## スケーリングの全体像

ここまで学んだスケーリングの仕組みを、他のトピックとの関係を含めてまとめます

### スケーリングと他の仕組みの連携

HPA が Pod の数を増やしたとき、以下の仕組みが連携して動作します

<strong>Scheduler（03-scheduling）</strong>

新しく作成された Pod は、Scheduler によって適切なノードに配置されます

リソース要求に基づくフィルタリングとスコアリングで、最適なノードが選ばれます

<strong>Service / EndpointSlice（04-service-discovery）</strong>

新しい Pod が起動すると、EndpointSlice に自動追加されます

kube-proxy がトラフィック転送のルールを更新し、新しい Pod にもトラフィックが振り分けられます

<strong>セルフヒーリング（05-self-healing）</strong>

HPA がレプリカ数を 5 に変更した後、Pod が 1 つ停止すれば、セルフヒーリングが 5 つに戻します

HPA が設定したあるべき状態を、セルフヒーリングが維持します

### スケーリングの流れ

全体の流れを見てみましょう

```
負荷が増加する
  │
  ▼
HPA がメトリクスを検知（CPU 使用率が目標を超えた）
  │
  ▼
HPA がレプリカ数を増やす（あるべき状態を変更）
  │
  ▼
Reconciliation Loop が差分を検知
  │
  ▼
新しい Pod を作成
  │
  ▼
Scheduler が適切なノードに配置
  │
  ▼
kubelet が Pod を起動
  │
  ▼
EndpointSlice が更新され、Service を通じてトラフィックが転送される
  │
  ▼
負荷が分散され、CPU 使用率が目標値に近づく
```

### Compose の限界への回答

[01-orchestration](./01-orchestration.md) で挙げた Compose の限界を振り返ります

| Compose の制約             | Kubernetes の解決策                                        |
| -------------------------- | ---------------------------------------------------------- |
| 手動でのスケーリング       | HPA がメトリクスに基づいて自動でレプリカ数を調整する       |
| 単一マシンでのスケーリング | 複数ノードにまたがって Pod を配置できる                    |
| 負荷に応じた自動調整がない | HPA のアルゴリズムが負荷に応じて動的にレプリカ数を計算する |

---

## 次のトピックへ

このトピックでは、以下のことを学びました

- スケーリングには水平（Pod の数を変える）と垂直（Pod のリソースを変える）の 2 つの方向があり、Kubernetes は水平スケーリングを中心に設計されている
- 手動スケーリングはレプリカ数を変更するだけで機能するが、24 時間の監視や即応性に限界がある
- HPA がメトリクスに基づいてレプリカ数を自動で調整し、セルフヒーリングの「あるべき状態の維持」を「あるべき状態の動的変更」に拡張する
- HPA のアルゴリズムは、現在のメトリクス値と目標値の比率から必要なレプリカ数を計算する
- スケールアウトは即応的に、スケールインは安定化ウィンドウで慎重に行うことで、フラッピングを防ぐ

ここまでのトピックで、スケジューリング、サービスディスカバリ、セルフヒーリング、スケーリングと、Kubernetes の個別の仕組みを学んできました

しかし、これらの仕組みは個別に動いているのではなく、<strong>宣言的な構成管理</strong>によって統合されています

Kubernetes に対して「何を、どのような状態にしたいか」を宣言する方法とは何でしょうか？

宣言的アプローチと命令的アプローチには、どのような違いがあるのでしょうか？

アプリケーションの更新やロールバックは、どのように行われるのでしょうか？

次のトピック [07-declarative-management](./07-declarative-management.md) では、<strong>宣言的構成管理</strong>を学びます

ここまで個別に学んだ全メカニズムを、宣言的なマニフェストとして統合する仕組みを見ていきます

---

## 用語集

| 用語                                     | 説明                                                                                                      |
| ---------------------------------------- | --------------------------------------------------------------------------------------------------------- |
| スケーリング（Scaling）                  | システムの処理能力を負荷に応じて増減させること                                                            |
| 水平スケーリング（Horizontal Scaling）   | Pod の数を増減させることで処理能力を調整する方法                                                          |
| 垂直スケーリング（Vertical Scaling）     | Pod に割り当てるリソース（CPU やメモリ）を変更することで処理能力を調整する方法                            |
| スケールアウト（Scale Out）              | Pod の数を増やすこと。水平スケーリングの拡張方向                                                          |
| スケールイン（Scale In）                 | Pod の数を減らすこと。水平スケーリングの縮小方向                                                          |
| HPA（Horizontal Pod Autoscaler）         | メトリクスに基づいて Pod のレプリカ数を自動で調整するコントローラ                                         |
| メトリクス（Metrics）                    | システムの状態を数値化した測定値。CPU 使用率、メモリ使用量、リクエスト数など                              |
| Metrics API                              | Pod やノードのリソース使用量を提供する Kubernetes の API                                                  |
| metrics-server                           | Metrics API の実装。各ノードの kubelet からリソース使用量を収集する                                       |
| リソースメトリクス（Resource Metrics）   | Pod の CPU やメモリの使用率を表すメトリクス                                                               |
| カスタムメトリクス（Custom Metrics）     | アプリケーション固有のメトリクス。リクエスト数やキューの長さなど                                          |
| 許容範囲（Tolerance）                    | メトリクス比率がこの範囲内であればスケーリングを行わない閾値。デフォルトは 0.1（10%）                     |
| 安定化ウィンドウ（Stabilization Window） | スケーリング決定時に過去の推奨値を参照する期間。フラッピングを防ぐための仕組み                            |
| フラッピング（Flapping）                 | Pod の数が短期間で増減を繰り返す不安定な状態                                                              |
| VPA（Vertical Pod Autoscaler）           | Pod のリソース要求を自動で調整する仕組み。Kubernetes のコア機能ではなく、追加コンポーネントとして導入する |
| レプリカ数（Replica Count）              | 同じ構成の Pod を何個動かすかを指定する数                                                                 |

---

## 参考資料

このページの内容は、以下のソースに基づいています

<strong>Horizontal Pod Autoscaler</strong>

- [Horizontal Pod Autoscaling](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)
  - HPA の仕組み、アルゴリズム、安定化ウィンドウの公式ドキュメント

- [HorizontalPodAutoscaler Walkthrough](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/)
  - HPA の動作を順を追って説明する公式ドキュメント

<strong>メトリクス</strong>

- [Resource metrics pipeline](https://kubernetes.io/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/)
  - metrics-server と Metrics API の公式ドキュメント

<strong>ワークロード管理</strong>

- [Deployments](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/)
  - Deployment のレプリカ数管理の公式ドキュメント
