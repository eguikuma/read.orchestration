<div align="right">
<img src="https://img.shields.io/badge/AI-ASSISTED_STUDY-3b82f6?style=for-the-badge&labelColor=1e293b&logo=bookstack&logoColor=white" alt="AI Assisted Study" />
</div>

# 05-self-healing：セルフヒーリング

## はじめに

前のトピック [04-service-discovery](./04-service-discovery.md) では、Pod 同士がどのように互いを発見し、安定した通信を実現するかを学びました

Service が安定した ClusterIP を提供し、EndpointSlice が Pod の IP アドレスを自動追跡し、kube-proxy がトラフィックを実際の Pod に転送する仕組みを確認しました

Pod の配置（スケジューリング）と発見（サービスディスカバリ）の仕組みが揃いました

しかし、ここで新たな疑問が生まれます

Pod やノードに障害が発生したとき、システムはどうやって自動的に復旧するのでしょうか？

あるべき状態と実際の状態にずれが生じたとき、それを検知する仕組みはどう動くのでしょうか？

Pod が「正常に動いている」とは、何をもって判断するのでしょうか？

このトピックでは、<strong>セルフヒーリング</strong>の仕組みを学びます

障害を自動で検知し、あるべき状態に向けて自動で復旧するメカニズムを見ていきます

---

## 日常の例え

セルフヒーリングの考え方を、日常の例えで見てみましょう

<strong>セルフヒーリング = ビルの自動管理システム</strong>

大きなオフィスビルには、各フロアにセンサーが設置されています

空調が壊れたフロアがあれば、センサーが異常を検知し、自動で予備の空調に切り替えます

あるフロア全体が停電になれば、そのフロアの機能を別のフロアに移します

管理人が1つ1つ巡回して手作業で対応する必要はありません

Kubernetes のセルフヒーリングも同じ仕組みです

コンテナの異常、Pod の障害、ノードのダウンを自動で検知し、あるべき状態に戻すために自動で行動します

<strong>ヘルスチェック = 3 種類の健康診断</strong>

会社の健康管理を想像してみましょう

<strong>生存確認（Liveness）</strong>は「社員が仕事をしているか」の確認です

出勤しているが仕事をしていなければ、一度退勤させてから再出勤させます（コンテナを再起動します）

<strong>受入準備確認（Readiness）</strong>は「社員が仕事を受けられる状態か」の確認です

会議中で手が離せない社員には新しい仕事を割り振りません（トラフィックを転送しません）

会議が終われば、再び仕事を割り振ります

<strong>起動確認（Startup）</strong>は「新入社員の研修が終わったか」の確認です

研修中はまだ仕事の判定をしません（Liveness チェックを始めません）

研修が終わってから、通常の確認が始まります

---

## このページで学ぶこと

このページでは、以下の概念を学びます

<strong>あるべき状態と自動復旧</strong>

- <strong>Reconciliation Loop の深掘り</strong>
  - あるべき状態と実際の状態を比較し、差分を自動修正する仕組みの具体的な動作
- <strong>コントローラによる障害検知と修復</strong>
  - ReplicaSet コントローラ、Node コントローラ等がどう障害に対応するか

<strong>Pod レベルのセルフヒーリング</strong>

- <strong>コンテナの再起動</strong>
  - kubelet がコンテナの異常終了を検知し、自動で再起動する仕組み
- <strong>再起動ポリシー（restartPolicy）</strong>
  - Always、OnFailure、Never の 3 種類のポリシー

<strong>ヘルスチェック（Probe）</strong>

- <strong>Liveness Probe</strong>
  - コンテナが生きているかを確認し、応答がなければ再起動する仕組み
- <strong>Readiness Probe</strong>
  - コンテナがトラフィックを受け入れる準備ができているかを確認する仕組み
- <strong>Startup Probe</strong>
  - コンテナの起動が完了したかを確認する仕組み

<strong>ノード障害への対応</strong>

- <strong>ノードの監視とハートビート</strong>
  - ノードの正常性を確認する仕組み
- <strong>ノード障害時の Pod 退避</strong>
  - ノードが応答しなくなった場合の自動復旧

---

## 目次

1. [あるべき状態の維持（復習と深掘り）](#あるべき状態の維持復習と深掘り)
2. [Pod レベルのセルフヒーリング](#pod-レベルのセルフヒーリング)
3. [ヘルスチェック（Probe）](#ヘルスチェックprobe)
4. [Probe の実行方法](#probe-の実行方法)
5. [ノード障害とリカバリ](#ノード障害とリカバリ)
6. [セルフヒーリングの全体像](#セルフヒーリングの全体像)
7. [次のトピックへ](#次のトピックへ)
8. [用語集](#用語集)
9. [参考資料](#参考資料)

---

## あるべき状態の維持（復習と深掘り）

[02-architecture](./02-architecture.md) で、「あるべき状態（Desired State）」と「Reconciliation Loop（調整ループ）」の概念を導入しました

ここでは、セルフヒーリングの文脈でこの仕組みを深掘りします

### Reconciliation Loop の復習

Reconciliation Loop は、以下の 3 ステップを<strong>継続的に繰り返す</strong>サイクルです

```
1. 観察（Observe）：実際の状態を確認する
2. 比較（Diff）    ：あるべき状態と実際の状態を比較する
3. 調整（Act）     ：差分があれば、あるべき状態に近づける操作を実行する
```

このサイクルは一度きりではなく、何度も繰り返されます

変更があっても、障害が起きても、調整ループが継続的に動くことで、システムは常にあるべき状態に向かって収束します

### セルフヒーリングとは

<strong>セルフヒーリング</strong>とは、<strong>障害を検知し、あるべき状態に戻すための調整を自動で行う仕組み</strong>です

これは Reconciliation Loop の自然な結果です

あるべき状態が「Pod を 3 つ動かす」と宣言されているとき、Pod が 1 つ停止すれば実際の状態は 2 つになります

調整ループがこの差分を検知し、新しい Pod を自動で作成して 3 つに戻します

管理者が手動で介入する必要はありません

### コントローラの役割

セルフヒーリングを実行するのは、コントロールプレーンの<strong>コントローラ</strong>です

各コントローラは、特定の種類のリソースを担当し、そのリソースのあるべき状態を維持する責任を持ちます

セルフヒーリングに関わる主要なコントローラは以下です

| コントローラ            | 担当             | セルフヒーリングの役割                                    |
| ----------------------- | ---------------- | --------------------------------------------------------- |
| ReplicaSet コントローラ | Pod のレプリカ数 | Pod の数が不足すれば新しい Pod を作成し、過剰なら削除する |
| Node コントローラ       | ノードの状態     | ノードの応答を監視し、応答がないノードを検知する          |
| Deployment コントローラ | Deployment       | ReplicaSet を管理し、ローリングアップデートを制御する     |

### ReplicaSet コントローラの動作

ReplicaSet コントローラの動作を、障害発生時の流れで見てみましょう

<strong>正常な状態</strong>

```
あるべき状態：Pod 3 つ
実際の状態  ：Pod 3 つ（ノード 1, ノード 2, ノード 3）
差分        ：なし
アクション  ：何もしない
```

<strong>Pod が 1 つ停止した場合</strong>

```
あるべき状態：Pod 3 つ
実際の状態  ：Pod 2 つ（ノード 1, ノード 3）← ノード 2 の Pod が停止
差分        ：Pod が 1 つ不足
アクション  ：新しい Pod を作成する
```

ReplicaSet コントローラは、Pod が 1 つ不足していることを検知し、API Server に対して新しい Pod の作成をリクエストします

作成された Pod は Scheduler によって適切なノードに割り当てられ、kubelet によって起動されます

Service の EndpointSlice も自動更新され、新しい Pod へのトラフィック転送が始まります

このように、スケジューリング、サービスディスカバリ、セルフヒーリングが連携することで、障害からの復旧がすべて自動で行われます

---

## Pod レベルのセルフヒーリング

セルフヒーリングには複数のレベルがあります

まず、最も基本的な<strong>Pod レベルのセルフヒーリング</strong>を見てみましょう

### コンテナの異常終了と再起動

Pod の中で動いているコンテナが異常終了した場合、ノード上の<strong>kubelet</strong>がこれを検知し、コンテナを自動的に再起動します

これは、ReplicaSet コントローラが新しい Pod を作成する仕組みとは別の仕組みです

ReplicaSet コントローラは Pod の数を管理しますが、kubelet は Pod の中のコンテナの状態を管理します

```
Pod レベルのセルフヒーリング（kubelet）
  コンテナが異常終了 → kubelet が検知 → 同じ Pod 内でコンテナを再起動

レプリカレベルのセルフヒーリング（ReplicaSet コントローラ）
  Pod 自体が消失 → コントローラが検知 → 新しい Pod を作成
```

### 再起動ポリシー（restartPolicy）

Pod には<strong>再起動ポリシー（restartPolicy）</strong>が設定されており、コンテナの再起動をどう扱うかを決めます

| ポリシー  | 動作                                                            |
| --------- | --------------------------------------------------------------- |
| Always    | コンテナが終了すると、終了理由に関わらず常に再起動する          |
| OnFailure | コンテナが異常終了（終了コードが 0 以外）した場合のみ再起動する |
| Never     | コンテナが終了しても再起動しない                                |

<strong>Always</strong>がデフォルトのポリシーです

Web サーバーのように常時稼働が必要なアプリケーションでは Always を使い、バッチ処理のように一度実行すれば完了するタスクでは OnFailure や Never を使います

### 再起動のバックオフ

コンテナが繰り返し異常終了する場合、kubelet は再起動の間隔を<strong>指数関数的に延長</strong>します

再起動の間隔は 10 秒から始まり、20 秒、40 秒と広がり、最大 5 分まで延長されます

この状態は<strong>CrashLoopBackOff</strong>と呼ばれます

CrashLoopBackOff は、アプリケーションに根本的な問題（設定ミス、依存サービスの不在など）があることを示しています

kubelet が再起動を繰り返しても解決しないため、間隔を広げて無駄なリソース消費を避けます

コンテナが正常に 10 分間動作すると、バックオフのタイマーはリセットされます

---

## ヘルスチェック（Probe）

コンテナが動いていても、アプリケーションが正常に機能しているとは限りません

たとえば、コンテナのプロセスは生きているが、内部でデッドロック（処理が永久に停止する状態）が発生し、リクエストに応答できなくなることがあります

このような状態を検知するために、Kubernetes は<strong>Probe（プローブ）</strong>と呼ばれるヘルスチェックの仕組みを提供しています

### 3 種類の Probe

Kubernetes には 3 種類の Probe があります

それぞれ目的と失敗時の動作が異なります

| Probe           | 目的                                                           | 失敗時の動作                                                                                                 |
| --------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------ |
| Liveness Probe  | コンテナが生きているかを確認する                               | コンテナを再起動する                                                                                         |
| Readiness Probe | コンテナがトラフィックを受け入れる準備ができているかを確認する | Service の EndpointSlice から Pod を除外する（再起動はしない）                                               |
| Startup Probe   | コンテナの起動が完了したかを確認する                           | 成功するまで Liveness Probe と Readiness Probe を無効にする。failureThreshold に達するとコンテナを再起動する |

### Liveness Probe（生存確認）

<strong>Liveness Probe</strong>は、<strong>コンテナが正常に動いているかを定期的に確認する</strong>仕組みです

Liveness Probe が失敗すると、kubelet はコンテナを再起動します

典型的な使用例は、デッドロックの検出です

アプリケーションのプロセスは生きているが、内部で処理が停止し、リクエストに応答できない状態を検知します

kubelet がコンテナを再起動することで、アプリケーションが回復する可能性があります

### Readiness Probe（受入準備確認）

<strong>Readiness Probe</strong>は、<strong>コンテナがトラフィックを受け入れる準備ができているかを定期的に確認する</strong>仕組みです

Readiness Probe が失敗すると、その Pod は Service の EndpointSlice から一時的に除外されます

トラフィックが転送されなくなりますが、<strong>コンテナは再起動されません</strong>

ここが Liveness Probe との大きな違いです

Readiness Probe は「一時的にトラフィックを受けられない状態」に対応します

たとえば、アプリケーションが起動時に大量のデータを読み込む必要がある場合、読み込みが完了するまで Readiness Probe を失敗させることで、準備が整っていない Pod にトラフィックが送られることを防ぎます

読み込みが完了し、Readiness Probe が成功すれば、Pod は再び EndpointSlice に追加され、トラフィックが転送されます

### Startup Probe（起動確認）

<strong>Startup Probe</strong>は、<strong>コンテナの起動が完了したかを確認する</strong>仕組みです

Startup Probe が設定されている場合、<strong>Startup Probe が成功するまで、Liveness Probe と Readiness Probe は実行されません</strong>

これは、起動に時間がかかるアプリケーションを保護するためです

たとえば、起動に 60 秒かかるアプリケーションに Liveness Probe が設定されているとします

Startup Probe がなければ、起動中に Liveness Probe が失敗し、kubelet がコンテナを再起動してしまいます

再起動のたびに起動プロセスがやり直しになり、アプリケーションは永遠に起動できません

Startup Probe を設定することで、起動完了まで Liveness Probe の実行を延期し、この問題を防ぎます

Startup Probe が failureThreshold に達して失敗した場合、kubelet はコンテナを強制終了し、再起動ポリシーに従って再起動します

これにより、起動に根本的な問題がある場合は、再起動を試みることができます

### Compose のヘルスチェックとの比較

前のシリーズでコンテナの管理を学んだ方は、Compose のヘルスチェックを思い出すかもしれません

Compose のヘルスチェックは 1 種類のみで、「コンテナが正常かどうか」を判定します

Kubernetes は、これを 3 種類に分けることで、より細かい制御を可能にしています

| 判定                       | Compose                        | Kubernetes      |
| -------------------------- | ------------------------------ | --------------- |
| コンテナが生きているか     | ヘルスチェック（1 種類で兼用） | Liveness Probe  |
| トラフィックを受けられるか | 区別なし                       | Readiness Probe |
| 起動が完了したか           | 区別なし                       | Startup Probe   |

Compose では「正常かどうか」の二択でしたが、Kubernetes では「生きているが、まだトラフィックは受けられない」「起動中だから、まだ判定しない」といった中間状態を扱えます

---

## Probe の実行方法

Probe は、コンテナの正常性をどうやって確認するのでしょうか

Kubernetes は、Probe の実行方法として主に 3 つの方法を提供しています

### HTTP GET

指定したパスとポートに HTTP GET リクエストを送信します

HTTP ステータスコードが 200 以上 400 未満であれば成功、それ以外は失敗と判定します

```yaml
livenessProbe:
  httpGet:
    path: /healthz
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 10
```

Web アプリケーションで最も一般的に使われる方法です

アプリケーションにヘルスチェック用のエンドポイント（`/healthz` など）を用意し、そこにリクエストを送ります

### TCP Socket

指定したポートに TCP 接続を試みます

接続が確立できれば成功、できなければ失敗と判定します

```yaml
livenessProbe:
  tcpSocket:
    port: 3306
  initialDelaySeconds: 15
  periodSeconds: 10
```

HTTP エンドポイントを持たないアプリケーション（データベースなど）に適しています

### コマンド実行（exec）

コンテナ内で指定したコマンドを実行します

コマンドの終了コードが 0 であれば成功、0 以外であれば失敗と判定します

```yaml
livenessProbe:
  exec:
    command:
      - cat
      - /tmp/healthy
  initialDelaySeconds: 5
  periodSeconds: 5
```

HTTP や TCP では判定できない、アプリケーション固有の正常性確認に使われます

### Probe の設定パラメータ

Probe の動作は、以下のパラメータで制御します

| パラメータ          | 説明                                                        | デフォルト値 |
| ------------------- | ----------------------------------------------------------- | ------------ |
| initialDelaySeconds | コンテナ起動後、最初の Probe を実行するまでの待ち時間（秒） | 0            |
| periodSeconds       | Probe を実行する間隔（秒）                                  | 10           |
| timeoutSeconds      | Probe のタイムアウト（秒）                                  | 1            |
| successThreshold    | 失敗後、成功と判定するために必要な連続成功回数              | 1            |
| failureThreshold    | 失敗と判定するために必要な連続失敗回数                      | 3            |

たとえば、`failureThreshold: 3` かつ `periodSeconds: 10` の場合、Probe が 3 回連続で失敗する（約 30 秒間応答がない）とアクションが実行されます

Liveness Probe であればコンテナが再起動され、Readiness Probe であれば Pod が EndpointSlice から除外されます

---

## ノード障害とリカバリ

ここまで、Pod 内のコンテナレベルでのセルフヒーリングを見てきました

しかし、Pod が動いているノード自体がダウンした場合はどうなるでしょうか

### ノードの監視とハートビート

Kubernetes は、各ノードの正常性を<strong>ハートビート</strong>で確認しています

ハートビートとは、ノードがコントロールプレーンに対して定期的に送信する「生存報告」です

ハートビートには 2 つの形式があります

| 形式                     | 説明                                                                                        |
| ------------------------ | ------------------------------------------------------------------------------------------- |
| ノードステータスの更新   | ノードの状態（CPU、メモリ、ディスクの状態、ネットワークの状態など）を API Server に報告する |
| Lease オブジェクトの更新 | `kube-node-lease` Namespace に Lease オブジェクトを作成し、定期的に更新する                 |

Lease オブジェクトは軽量な「心拍」として機能します

ノードステータスの更新は多くの情報を含むため頻繁に送ると負荷がかかりますが、Lease オブジェクトの更新は軽量で、ノードの可用性を効率的に確認できます

### Node コントローラによる障害検知

コントロールプレーンの<strong>Node コントローラ</strong>は、ノードのハートビートを監視しています

ノードからのハートビートが一定期間（デフォルトでは 50 秒）途絶えると、Node コントローラはそのノードの状態を<strong>NotReady</strong>に変更します

### ノード障害時の Pod 退避

ノードが NotReady になると、そのノード上の Pod は以下の流れで退避されます

<strong>1. Taint の付与</strong>

Node コントローラは、NotReady になったノードに `node.kubernetes.io/not-ready` という<strong>Taint（テイント）</strong>を付与します

Taint とは、ノードに「このノードには問題がある」というマークを付ける仕組みです

[03-scheduling](./03-scheduling.md) で学んだように、Taint が付いたノードには、対応する Toleration を持たない Pod はスケジュールされません

<strong>2. Pod の退避</strong>

既にそのノード上で動いている Pod のうち、この Taint に対する Toleration を持たない Pod は退避対象になります

Kubernetes はデフォルトで、Pod に 5 分間（300 秒）の Toleration を自動付与しています

つまり、ノードが NotReady になってから 5 分間は Pod が退避されず、ノードの復旧を待ちます

5 分経ってもノードが復旧しなければ、Pod は退避され、ReplicaSet コントローラによって別のノードに新しい Pod が作成されます

<strong>3. 退避の流れ</strong>

```
ノードのハートビートが途絶える
  │
  ▼
Node コントローラが NotReady を検知
  │
  ▼
Taint（node.kubernetes.io/not-ready）を付与
  │
  ▼
Toleration の猶予時間（デフォルト 300 秒）を待つ
  │
  ▼
Pod が退避される
  │
  ▼
ReplicaSet コントローラが Pod の不足を検知
  │
  ▼
新しい Pod を作成（Scheduler が別のノードに配置）
  │
  ▼
Service の EndpointSlice が更新され、新しい Pod にトラフィックが転送される
```

この流れが完了するまでの間、Service は残りの正常な Pod にトラフィックを振り分けます

Readiness Probe によって NotReady な Pod は既に EndpointSlice から除外されているため、障害が発生した Pod にはトラフィックが送られません

---

## セルフヒーリングの全体像

ここまで学んだ仕組みを、3 つのレベルに整理してまとめます

### 3 層のセルフヒーリング

Kubernetes のセルフヒーリングは、3 つのレベルで構成されています

<strong>レベル 1：コンテナレベル（kubelet + Probe）</strong>

Pod 内のコンテナが異常終了したり、Liveness Probe に失敗した場合、kubelet がコンテナを再起動します

Pod 自体は同じノード上に残り、コンテナだけが再起動されます

最も軽量で高速な回復手段です

<strong>レベル 2：レプリカレベル（ReplicaSet コントローラ）</strong>

Pod 自体が消失した場合（ノード障害による退避を含む）、ReplicaSet コントローラが新しい Pod の作成をリクエストします

Scheduler が新しい Pod を適切なノードに配置し、kubelet が起動します

<strong>レベル 3：ノードレベル（Node コントローラ）</strong>

ノードが応答しなくなった場合、Node コントローラがノードを NotReady にマークし、Taint を付与します

そのノード上の Pod は退避され、レベル 2 の仕組みで別のノードに再作成されます

### 各レベルの比較

| レベル         | 担当                    | 対象                                    | 復旧方法           | 速度           |
| -------------- | ----------------------- | --------------------------------------- | ------------------ | -------------- |
| コンテナレベル | kubelet                 | コンテナの異常終了、Liveness Probe 失敗 | コンテナの再起動   | 速い（秒単位） |
| レプリカレベル | ReplicaSet コントローラ | Pod の消失                              | 新しい Pod の作成  | 中程度         |
| ノードレベル   | Node コントローラ       | ノードの応答停止                        | Pod の退避と再作成 | 遅い（分単位） |

### セルフヒーリングとあるべき状態

セルフヒーリングは、あるべき状態を実現するための仕組みの 1 つです

管理者が「Web サーバーの Pod を 3 つ動かす」と宣言すれば、以下のすべての障害から自動で復旧します

- コンテナが異常終了した → kubelet が再起動
- Pod が消失した → ReplicaSet コントローラが新しい Pod を作成
- ノードがダウンした → Node コントローラが Pod を退避、別ノードで再作成

管理者はあるべき状態を宣言するだけで、障害対応の手順を記述する必要はありません

Kubernetes が Reconciliation Loop を通じて、自動的にあるべき状態に戻します

---

## 次のトピックへ

このトピックでは、以下のことを学びました

- セルフヒーリングは Reconciliation Loop の自然な結果であり、あるべき状態と実際の状態の差分を自動で修正する仕組みである
- Pod レベルでは kubelet がコンテナの異常終了を検知し、再起動ポリシーに従って自動再起動する
- Liveness Probe、Readiness Probe、Startup Probe の 3 種類のヘルスチェックが、コンテナの状態をきめ細かく判定する
- ノード障害時は Node コントローラがハートビートの途絶を検知し、Taint を付与して Pod を退避させる
- コンテナレベル、レプリカレベル、ノードレベルの 3 層構造で、さまざまな障害に自動で対応する

セルフヒーリングにより、あるべき状態を「維持する」仕組みが揃いました

ここで、新たな疑問が生まれます

あるべき状態が「Pod を 3 つ動かす」のとき、セルフヒーリングは Pod を 3 つに維持します

しかし、アクセスが急増して 3 つでは処理しきれなくなったらどうなるでしょうか？

あるべき状態そのもの、つまり Pod の数を「3 つ」から「10 個」に動的に変更する仕組みはあるのでしょうか？

負荷が下がったとき、再び Pod の数を減らすことはできるのでしょうか？

次のトピック [06-scaling](./06-scaling.md) では、<strong>スケーリング</strong>の仕組みを学びます

あるべき状態の「維持」から「動的な変更」へと進みます

---

## 用語集

| 用語                              | 説明                                                                                                                  |
| --------------------------------- | --------------------------------------------------------------------------------------------------------------------- |
| セルフヒーリング（Self-Healing）  | 障害を自動で検知し、あるべき状態に戻すための調整を自動で行う仕組み                                                    |
| Reconciliation Loop（調整ループ） | 観察→比較→調整のサイクルを継続的に繰り返し、実際の状態をあるべき状態に収束させるメカニズム                            |
| コントローラ（Controller）        | 特定のリソースのあるべき状態を維持する責任を持つ制御ループ。Controller Manager に内包される                           |
| ReplicaSet コントローラ           | 指定された数の Pod レプリカを維持するコントローラ。Pod の数が不足すれば作成し、過剰なら削除する                       |
| Node コントローラ                 | ノードの状態を監視し、応答がないノードを検知するコントローラ                                                          |
| 再起動ポリシー（restartPolicy）   | コンテナの終了時に kubelet がどう対応するかを決めるポリシー。Always、OnFailure、Never の 3 種類がある                 |
| CrashLoopBackOff                  | コンテナが繰り返し異常終了し、kubelet が再起動の間隔を指数関数的に延長している状態                                    |
| Probe（プローブ）                 | kubelet がコンテナに対して定期的に実行するヘルスチェック                                                              |
| Liveness Probe                    | コンテナが生きているかを確認する Probe。失敗するとコンテナが再起動される                                              |
| Readiness Probe                   | コンテナがトラフィックを受け入れる準備ができているかを確認する Probe。失敗すると EndpointSlice から Pod が除外される  |
| Startup Probe                     | コンテナの起動が完了したかを確認する Probe。成功するまで Liveness Probe と Readiness Probe が無効になる               |
| ハートビート（Heartbeat）         | ノードがコントロールプレーンに対して定期的に送信する生存報告。ノードステータスの更新と Lease オブジェクトの更新がある |
| Lease オブジェクト                | ノードの可用性を効率的に確認するための軽量なリソース。kube-node-lease Namespace に作成される                          |
| NotReady                          | ノードがハートビートを送信しなくなった場合にマークされる状態                                                          |
| Taint（テイント）                 | ノードに「問題がある」ことをマークする仕組み。対応する Toleration を持たない Pod はスケジュールされない               |
| Toleration（トレレーション）      | 特定の Taint を許容するための Pod 側の設定                                                                            |
| デッドロック（Deadlock）          | 複数の処理が互いの完了を待ち合い、どの処理も進めなくなる状態                                                          |

---

## 参考資料

このページの内容は、以下のソースに基づいています

<strong>Pod のライフサイクル</strong>

- [Pod Lifecycle](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/)
  - Pod のフェーズ、コンテナの状態、再起動ポリシーの公式ドキュメント

<strong>ヘルスチェック</strong>

- [Configure Liveness, Readiness and Startup Probes](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/)
  - Liveness Probe、Readiness Probe、Startup Probe の設定方法の公式ドキュメント

<strong>ノード管理</strong>

- [Nodes](https://kubernetes.io/docs/concepts/architecture/nodes/)
  - Node コントローラ、ハートビート、ノード障害時の動作の公式ドキュメント

<strong>ワークロード管理</strong>

- [ReplicaSet](https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/)
  - ReplicaSet の仕組みの公式ドキュメント
