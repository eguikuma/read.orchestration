---
layout: default
title: オーケストレーションとは何か
---

# [01-orchestration：オーケストレーションとは何か](#orchestration) {#orchestration}

## [はじめに](#introduction) {#introduction}

前のシリーズでは、コンテナの仕組みを学び、Compose で複数のコンテナをまとめて定義・管理する方法を学びました

1 つのファイルに Web サーバー、データベース、キャッシュなどを定義し、1 つのコマンドで起動できる便利さを体験しました

しかし、Compose にはいくつかの限界があります

Compose は 1 台のマシンでしか動きません

利用者が増えて 1 台では処理しきれなくなったとき、コンテナを複数のマシンに分散させる仕組みが Compose にはありません

コンテナが異常終了しても、Compose にできるのは単純な再起動だけです

コンテナの数を増やしたいとき、手作業でレプリカ数を変更する必要があります

では、複数のマシンにまたがるコンテナ群を、どうやって管理するのでしょうか？

コンテナが障害で停止したとき、どうやって自動的に復旧するのでしょうか？

このトピックでは、Compose の限界から出発し、<strong>コンテナオーケストレーション</strong>がなぜ必要で、何を解決するのかを学びます

---

## [日常の例え](#everyday-analogy) {#everyday-analogy}

オーケストレーションの考え方を、日常の例えで見てみましょう

<strong>Compose = 1 人で切り盛りする小さなレストラン</strong>

店長が 1 人で、注文を受け、調理し、配膳するレストランを想像してください

少人数のお客さんなら問題なくこなせます

しかし、お客さんが増えると 1 人では限界が来ます

店を増やしたいと思っても、この店長は 1 つの店舗でしか働けません

Compose も同じで、1 台のマシン内でしかコンテナを管理できません

<strong>オーケストレーション = レストランチェーンの本部</strong>

チェーン本部は、複数の店舗を束ねて管理します

本部が「この地域に新しい店舗を出す」「混雑している店舗にスタッフを増やす」「閉まってしまった店舗の代わりに近くの店舗をオープンする」と指示を出します

各店舗の店長は本部の指示に従い、自分の店を運営します

コンテナオーケストレーションも同じで、<strong>複数のマシンにまたがるコンテナ群を一元的に管理する仕組み</strong>です

<strong>Kubernetes の意味</strong>

Kubernetes という名前はギリシャ語で「操舵手」や「パイロット」を意味します

船の舵を取り、目的地に向かって船全体を導く人です

コンテナという「船」を、目的の状態に向かって導く仕組みとして、この名前がつけられました

---

## [このページで学ぶこと](#what-you-will-learn) {#what-you-will-learn}

このページでは、以下の概念を学びます

<strong>Compose の限界</strong>

- <strong>単一マシンの制約</strong>
  - 1 台のマシンでしか動かないことで生じる問題
- <strong>障害復旧の限界</strong>
  - 再起動ポリシーだけでは対応できない状況
- <strong>手動スケーリング</strong>
  - レプリカ数の手動変更とその問題

<strong>複数マシンの管理</strong>

- <strong>水平スケーリングの必要性</strong>
  - なぜマシンを増やす必要があるのか
- <strong>手動管理の限界</strong>
  - 複数マシンを人間が管理する場合の問題

<strong>オーケストレーションの概念</strong>

- <strong>オーケストレーションの定義</strong>
  - コンテナオーケストレーションとは何か
- <strong>オーケストレーションが解決する問題</strong>
  - Compose の限界がどう解決されるか
- <strong>Kubernetes の背景</strong>
  - なぜ Kubernetes が生まれたか

---

## [目次](#table-of-contents) {#table-of-contents}

1. [Compose の限界](#compose-limitations)
2. [複数マシンの管理という課題](#multi-machine-management-challenge)
3. [オーケストレーションとは何か](#what-is-orchestration)
4. [オーケストレーションが解決する問題](#problems-orchestration-solves)
5. [Kubernetes の誕生と背景](#kubernetes-origin)
6. [次のトピックへ](#next-topic)
7. [用語集](#glossary)
8. [参考資料](#references)

---

## [Compose の限界](#compose-limitations) {#compose-limitations}

Compose は、複数のコンテナを 1 つのファイルで定義し、まとめて管理する仕組みです

Web サーバー、データベース、キャッシュなどを 1 つの `compose.yaml` に定義すれば、1 つのコマンドですべて起動できます

開発環境やテスト環境では非常に便利ですが、本番環境で運用しようとすると、いくつかの限界が見えてきます

### [単一マシンの制約](#single-machine-constraint) {#single-machine-constraint}

Compose は、<strong>1 台のマシン（Docker ホスト）上でのみ</strong>コンテナを管理します

すべてのコンテナは同じマシンの CPU、メモリ、ネットワーク帯域を共有します

たとえば、Web アプリケーションの利用者が増えて 1 台のマシンでは処理しきれなくなったとしましょう

Compose には、コンテナを別のマシンに分散させる機能がありません

2 台目、3 台目のマシンを用意しても、Compose はそれらのマシンにコンテナを配置する方法を持っていません

{: .labeled}
| 状況 | Compose の対応 |
| -------------------------------- | ---------------------------------------- |
| 利用者増加で CPU が不足 | 対応できない（同じマシン内に限定） |
| メモリが不足 | 対応できない（マシンの物理メモリが上限） |
| 別のマシンにコンテナを配置したい | 対応できない（単一マシン前提） |
| マシンが故障した | すべてのコンテナが停止する |

最後の行が特に重要です

Compose で管理しているマシンが故障すると、そのマシン上の<strong>すべてのコンテナが失われます</strong>

1 台のマシンに依存することは、<strong>単一障害点（Single Point of Failure）</strong>になります

単一障害点とは、その 1 か所が壊れるとシステム全体が停止してしまう場所のことです

### [再起動ポリシーの限界](#restart-policy-limitation) {#restart-policy-limitation}

Compose では、コンテナが異常終了した場合の対応として<strong>再起動ポリシー</strong>を設定できます

{: .labeled}
| ポリシー | 動作 |
| ---------------- | ------------------------------ |
| `no` | 再起動しない（デフォルト） |
| `always` | 常に再起動する |
| `on-failure` | 異常終了時のみ再起動する |
| `unless-stopped` | 手動で停止しない限り再起動する |

一見すると `always` を設定すれば十分に見えますが、いくつかの問題があります

<strong>同じマシン上でしか再起動できない</strong>

コンテナが異常終了すると、Compose は<strong>同じマシン上で</strong>そのコンテナを再起動します

しかし、マシン自体が故障した場合、再起動する場所がありません

<strong>再起動の繰り返しに対応できない</strong>

コンテナが繰り返し異常終了する場合（たとえば、設定ファイルの誤りや依存サービスの障害）、Compose は何度も再起動を試みます

根本原因が解決されないまま、再起動と異常終了を延々と繰り返します

<strong>Liveness と Readiness の区別がない</strong>

Compose にはヘルスチェック機能があり、カスタムコマンド（たとえば HTTP リクエストの送信）でサービスの動作確認ができます

しかし、Compose のヘルスチェックは<strong>1 種類だけ</strong>です

オーケストレーションの世界では、ヘルスチェックを<strong>Liveness（生きているか）</strong>と<strong>Readiness（準備ができているか）</strong>の 2 つに分けます

たとえば、Web サーバーのコンテナが起動していても、内部の初期化が完了していなければリクエストには応答できません

Liveness チェックが失敗すればコンテナが再起動され、Readiness チェックが失敗すればトラフィックの送信先から外されます

Compose にはこの区別がないため、初期化中のコンテナにもリクエストが送られる可能性があります

### [手動スケーリング](#manual-scaling) {#manual-scaling}

Compose にはコンテナの数を増やす機能があります

`docker compose up --scale web=3` のように指定すると、`web` サービスのコンテナを 3 つ起動できます

しかし、この方法にはいくつかの制約があります

<strong>すべてのレプリカが同じマシン上で動く</strong>

3 つのレプリカを作っても、すべて同じマシン上で動きます

マシンの性能が限界に達していれば、レプリカを増やしても効果はありません

<strong>負荷に応じた自動スケーリングがない</strong>

アクセスが急増したとき、人間が気づいてレプリカ数を増やす必要があります

アクセスが減ったとき、人間が気づいてレプリカ数を減らす必要があります

この手動対応は、夜間や休日には間に合わないことがあります

### [サービスディスカバリの制約](#service-discovery-constraint) {#service-discovery-constraint}

Compose は、同じネットワーク内のコンテナに対して Docker の内部 DNS を提供します

サービス名（たとえば `db`）でコンテナにアクセスできるのは、この DNS のおかげです

しかし、この DNS は<strong>1 台のマシン内のコンテナに限定</strong>されています

別のマシン上で動いているコンテナをサービス名で見つけることはできません

複数のマシンにコンテナが分散している環境では、コンテナ同士がお互いをどうやって見つけるかという問題が生じます

これが<strong>サービスディスカバリ</strong>の課題です

### [Compose の限界のまとめ](#compose-limitations-summary) {#compose-limitations-summary}

{: .labeled}
| 限界 | 具体的な問題 |
| ----------------- | ------------------------------------------------ |
| 単一マシン | 1 台のマシンの性能が上限<br>マシン故障ですべて停止 |
| 再起動のみ | 同じマシンでの再起動だけ<br>マシン故障時は復旧不能 |
| 手動スケーリング | 人間が手動で増減<br>同じマシン上に限定 |
| ホスト内 DNS のみ | 別マシンのコンテナをサービス名で発見できない |

これらの限界は、Compose の設計が悪いわけではありません

Compose は<strong>1 台のマシン上での複数コンテナ管理</strong>を目的として設計されています

問題は、本番環境では 1 台のマシンでは足りないということです

---

## [複数マシンの管理という課題](#multi-machine-management-challenge) {#multi-machine-management-challenge}

Compose の限界を見てきました

では、なぜ複数のマシンが必要なのか、そして複数のマシンを管理するとはどういうことかを考えてみましょう

### [なぜ複数のマシンが必要か](#why-multiple-machines) {#why-multiple-machines}

マシンの性能が足りないとき、2 つのアプローチがあります

<strong>垂直スケーリング（スケールアップ）</strong>

マシンの性能を上げる方法です

CPU を高性能なものに交換する、メモリを増設する、ストレージを高速なものに変えるなどです

この方法は単純ですが、限界があります

{: .labeled}
| 垂直スケーリングの限界 |
| -------------------------------------------------------------------- |
| 1 台のマシンに搭載できる CPU やメモリには物理的な上限がある |
| 高性能なハードウェアは指数関数的に高価になる |
| マシンが 1 台のままなので、単一障害点は解消されない |
| スケールアップ時にマシンの停止（ダウンタイム）が必要になることがある |

<strong>水平スケーリング（スケールアウト）</strong>

マシンの台数を増やす方法です

性能が足りなければ、もう 1 台マシンを追加し、負荷を分散します

{: .labeled}
| 水平スケーリングの利点 |
| -------------------------------------------------------- |
| マシンを追加するだけで性能を向上できる |
| 1 台が故障しても、他のマシンが処理を続けられる |
| 一般的なハードウェアで構成できるため、コストを抑えやすい |
| マシンの追加時にダウンタイムが不要 |

本番環境では、<strong>水平スケーリング</strong>が一般的な選択です

しかし、マシンが増えると、新たな課題が生まれます

それが「複数マシンの管理」です

### [手動で複数マシンを管理する場合](#manual-multi-machine-management) {#manual-multi-machine-management}

マシンが 3 台あるとしましょう

Web アプリケーションのコンテナを、この 3 台に分散して配置したいとします

オーケストレーションの仕組みがない場合、以下のような手作業が必要になります

<strong>デプロイ（配置）</strong>

各マシンに SSH でログインし、コンテナイメージを取得し、コンテナを起動します

3 台なら 3 回、10 台なら 10 回、同じ作業を繰り返します

<strong>監視</strong>

各マシンの状態（CPU 使用率、メモリ使用量、コンテナの状態）を個別に確認します

あるマシンのコンテナが異常終了していないか、定期的にチェックする必要があります

<strong>障害復旧</strong>

あるマシンのコンテナが停止したことに気づいたら、そのマシンに SSH でログインし、コンテナを再起動します

マシン自体が故障した場合は、別のマシンにコンテナを手動で配置し直す必要があります

<strong>スケーリング</strong>

負荷が増えてマシンを追加する場合、新しいマシンを用意し、必要なソフトウェアをインストールし、コンテナを配置します

{: .labeled}
| 操作 | 手動管理の場合 |
| ------------ | ------------------------------------ |
| デプロイ | 各マシンに SSH して個別に実行 |
| 監視 | 各マシンを個別に確認 |
| 障害復旧 | 障害に気づき、手動で対応 |
| スケーリング | 新マシンを用意し、手動でセットアップ |
| 設定変更 | 各マシンで個別に設定を更新 |

### [手動管理の問題](#manual-management-problems) {#manual-management-problems}

マシンが 3 台なら手動管理はまだ現実的かもしれません

しかし、マシンが増えるにつれて問題は深刻になります

<strong>人為的ミス</strong>

マシンが増えるほど、設定の漏れや誤りが起きやすくなります

「あるマシンだけバージョンが古い」「あるマシンだけ設定が違う」といった<strong>構成のドリフト（意図しない差異）</strong>が発生します

<strong>復旧速度</strong>

人間が障害に気づき、対応を始めるまでに時間がかかります

手動での復旧には数分から数時間かかりますが、自動化された仕組みなら数秒で対応できます

<strong>スケールの限界</strong>

3 台のマシンなら管理できても、30 台、300 台となると人間の手作業では追いつきません

マシンの台数が増えるほど、自動化の必要性が高まります

この「複数マシンの管理を自動化する仕組み」が、これから学ぶ<strong>オーケストレーション</strong>です

---

## [オーケストレーションとは何か](#what-is-orchestration) {#what-is-orchestration}

### [言葉の由来](#etymology) {#etymology}

<strong>オーケストレーション（Orchestration）</strong>という言葉は、もともと音楽の用語です

オーケストラでは、指揮者が多数の楽器奏者を統率し、それぞれが異なるパートを演奏しながら 1 つの楽曲としてまとまるよう調整します

指揮者は自分では楽器を演奏しませんが、全体の調和を保ち、テンポを合わせ、各パートのバランスを取ります

コンピューティングの世界でも、多数のコンテナを統率し、それぞれが異なる役割を果たしながらシステム全体として機能するよう調整する仕組みを<strong>オーケストレーション</strong>と呼びます

### [コンテナオーケストレーションの定義](#container-orchestration-definition) {#container-orchestration-definition}

<strong>コンテナオーケストレーション</strong>とは、複数のマシン（<strong>クラスタ</strong>）にまたがるコンテナの配置、管理、スケーリング、ネットワーク接続を自動化する仕組みです

クラスタとは、複数のマシンを 1 つのまとまりとして管理するグループのことです

個々のマシンを<strong>ノード</strong>と呼びます

オーケストレーションの仕組みがあれば、管理者はクラスタに対して「何をしたいか」を伝えるだけで、あとはシステムが自動で対応します

### [オーケストレーションの 3 つの柱](#orchestration-three-pillars) {#orchestration-three-pillars}

コンテナオーケストレーションは、大きく 3 つの柱で構成されます

{: .labeled}
| 柱 | 役割 | 具体例 |
| ---------------- | ------------------------------------------------------ | ---------------------------------------------- |
| スケジューリング | どのコンテナをどのノードで動かすかを決める | CPU やメモリの空きに基づいて最適なノードを選ぶ |
| 管理 | コンテナの状態を監視し、異常があれば自動で対応する | コンテナの障害を検知し、別のノードで再作成する |
| ネットワーキング | コンテナ同士が、どのノードにいても通信できるようにする | サービス名でアクセスできる仕組みを提供する |

この 3 つの柱は、Compose の限界で挙げた問題に直接対応しています

<strong>スケジューリング</strong>は、複数のノードの中からコンテナの配置先を自動で決定します

手動で各マシンに SSH してコンテナを配置する必要がなくなります

<strong>管理</strong>は、コンテナの状態を継続的に監視し、障害を検知すると自動で復旧します

人間が気づくのを待つ必要がなくなります

<strong>ネットワーキング</strong>は、クラスタ全体でコンテナ同士が通信できる仕組みを提供します

単一マシンの Docker 内部 DNS に限定されなくなります

---

## [オーケストレーションが解決する問題](#problems-orchestration-solves) {#problems-orchestration-solves}

Compose の限界として挙げた問題が、オーケストレーションによってどう解決されるかを見てみましょう

{: .labeled}
| Compose の課題 | オーケストレーションの解決策 |
| --------------------------- | ---------------------------------------------------- |
| 単一マシンでの管理 | クラスタ（複数ノードの統合管理） |
| 同じマシンでの再起動のみ | セルフヒーリング（障害検知と別ノードでの自動復旧） |
| 手動スケーリング | 自動スケーリング（負荷に応じたコンテナ数の自動増減） |
| ホスト内の DNS のみ | クラスタワイドなサービスディスカバリ |
| ヘルスチェックが 1 種類のみ | Liveness / Readiness による詳細なヘルスチェック |

### [あるべき状態（Desired State）の考え方](#desired-state-concept) {#desired-state-concept}

オーケストレーションの仕組みには、もう 1 つ重要な考え方があります

それは<strong>「あるべき状態（Desired State）」を宣言する</strong>という考え方です

Compose でコンテナを管理する場合、操作は<strong>命令的（Imperative）</strong>です

「このコンテナを起動しろ」「このコンテナを停止しろ」という具体的な<strong>操作</strong>を指示します

一方、オーケストレーションでは<strong>宣言的（Declarative）</strong>な方法を取ります

「Web サーバーは常に 3 つ動いている状態にしてほしい」という<strong>あるべき状態</strong>を宣言します

具体的にどのノードでコンテナを起動するか、障害時にどう復旧するかは、オーケストレーションの仕組みが自動で判断します

この「あるべき状態を宣言し、システムがその状態を維持し続ける」という考え方は、このリポジトリ全体を通じて繰り返し登場する<strong>最も重要な概念</strong>です

次のトピックで、この考え方を技術的に詳しく学びます

---

## [Kubernetes の誕生と背景](#kubernetes-origin) {#kubernetes-origin}

コンテナオーケストレーションの仕組みはいくつか存在しますが、このリポジトリでは<strong>Kubernetes</strong>を主な例として使います

Kubernetes はオーケストレーションの事実上の標準ですが、このリポジトリの目的は Kubernetes の操作方法を覚えることではなく、<strong>オーケストレーションの原理</strong>を理解することです

Kubernetes を例に使うのは、原理を具体的に説明しやすいからです

### [Google の経験](#google-experience) {#google-experience}

Kubernetes の起源は、Google の社内システムにあります

Google は、社内で<strong>Borg</strong>と呼ばれるクラスタ管理システムを長年運用していました

Borg は、Google の検索エンジン、Gmail、YouTube など、大規模なサービスを動かす基盤です

2015 年に発表された論文（Verma et al., EuroSys 2015）によると、Borg は数万台のマシンで構成されるクラスタを管理し、数千のアプリケーションを動かしていました

Borg の核心的なアイデアは、<strong>「あるべき状態を宣言し、システムがその状態に向かって自律的に動く」</strong>というものです

管理者が「このアプリケーションを 50 個のインスタンスで動かしたい」と宣言すれば、Borg がクラスタの中から適切なマシンを選び、インスタンスを配置し、障害が起きれば自動で復旧しました

### [Borg から Kubernetes へ](#borg-to-kubernetes) {#borg-to-kubernetes}

Google は 2014 年に、Borg の経験を活かしたオープンソースプロジェクトとして Kubernetes を公開しました

Kubernetes は Borg をそのまま移植したものではありません

Burns et al.（ACM Queue 2016）では、Borg の運用から得た教訓が Kubernetes の設計にどう活かされたかが解説されています

Borg で学んだ重要な教訓のいくつかを紹介します

<strong>宣言的な構成</strong>

Borg の運用を通じて、命令的な操作（「このコンテナを起動しろ」）よりも、宣言的な構成（「この状態を維持しろ」）の方が、大規模システムの管理に適していることが分かりました

Kubernetes はこの考え方を核に設計されています

<strong>調整ループ（Reconciliation Loop）</strong>

あるべき状態と実際の状態を継続的に比較し、差分があれば自動で修正する仕組みです

Borg ではこの仕組みが大規模環境での信頼性を支えていました

Kubernetes でもこの仕組みが中心的な役割を果たします

<strong>API を中心とした設計</strong>

すべての操作を API を通じて行うことで、自動化やツールとの連携が容易になります

Kubernetes はすべての操作を RESTful API で提供しています

### [なぜ Kubernetes が標準となったか](#why-kubernetes-standard) {#why-kubernetes-standard}

Kubernetes は 2014 年の公開後、急速に普及しました

その理由はいくつかあります

<strong>オープンソース</strong>

Google が社内技術の知見をオープンソースとして公開したことで、誰でも利用・改善できるようになりました

<strong>CNCF による中立的なガバナンス</strong>

Kubernetes は 2015 年に<strong>CNCF（Cloud Native Computing Foundation）</strong>に移管されました

CNCF は特定の企業に依存しない中立的な組織で、Kubernetes の開発・運営を監督しています

これにより、特定のベンダーに縛られない信頼性が確保されました

<strong>拡張性の高い設計</strong>

Kubernetes は拡張ポイントが多く設計されており、コア機能を変更せずに機能を追加できます

これにより、さまざまな環境やユースケースに対応できます

---

## [次のトピックへ](#next-topic) {#next-topic}

このトピックでは、以下のことを学びました

- Compose は 1 台のマシン上でのコンテナ管理に限定されており、複数マシンへの分散、自動復旧、自動スケーリングの機能がない
- 本番環境では水平スケーリング（マシンの台数を増やす）が一般的だが、複数マシンの手動管理は人為的ミス、復旧速度、スケールの限界という問題を抱える
- コンテナオーケストレーションは、スケジューリング、管理、ネットワーキングの 3 つの柱で複数ノードのコンテナを自動管理する仕組みである
- 「あるべき状態（Desired State）を宣言し、システムがその状態を維持し続ける」という考え方がオーケストレーションの核心である
- Kubernetes は Google の Borg の経験を活かして設計されたオープンソースのオーケストレーション基盤であり、事実上の標準となっている

では、このオーケストレーションの仕組みは、具体的にどのような構造で動いているのでしょうか？

「あるべき状態を宣言する」とは、技術的にはどういう仕組みなのでしょうか？

複数のノードを管理する「コントロールプレーン」とは何でしょうか？

次のトピック [02-architecture](../02-architecture/) では、オーケストレーションの<strong>アーキテクチャ</strong>を学びます

コントロールプレーンとノードの分離、各コンポーネントの役割、そして「あるべき状態」を支える技術的な仕組みを見ていきます

---

## [用語集](#glossary) {#glossary}

{: .labeled}
| 用語 | 説明 |
| -------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- |
| Docker Compose | 複数のコンテナを 1 つのファイルで定義し、1 台のマシン上でまとめて管理するツール |
| 再起動ポリシー（Restart Policy） | コンテナが停止した際の再起動方法を定義する設定<br>`always`、`on-failure` などがある |
| 単一障害点（Single Point of Failure） | その箇所が故障するとシステム全体が停止してしまう、冗長性のない構成要素 |
| Liveness | コンテナ内のアプリケーションが生きている（動作している）かどうかを確認する仕組み |
| Readiness | コンテナ内のアプリケーションがリクエストを受け付ける準備ができているかどうかを確認する仕組み |
| サービスディスカバリ（Service Discovery） | 分散システムにおいて、サービス（コンテナ）がお互いの場所（IP アドレスなど）を動的に発見する仕組み |
| 垂直スケーリング（Vertical Scaling / Scale Up） | 1 台のマシンの性能（CPU、メモリなど）を向上させる方法 |
| 水平スケーリング（Horizontal Scaling / Scale Out） | マシンの台数を増やして処理能力を向上させる方法 |
| 構成のドリフト（Configuration Drift） | 複数のマシンの設定が時間の経過とともに意図せず異なってしまう現象 |
| オーケストレーション（Orchestration） | 複数のマシンにまたがるコンテナの配置、管理、スケーリング、ネットワーク接続を自動化する仕組み |
| クラスタ（Cluster） | 複数のマシンを 1 つのまとまりとして管理するグループ |
| ノード（Node） | クラスタを構成する個々のマシン |
| スケジューリング（Scheduling） | コンテナをどのノードで実行するかを決定するプロセス |
| セルフヒーリング（Self-healing） | コンテナの障害を検知し、自動的に復旧する仕組み |
| 命令的（Imperative） | 「何をするか」の具体的な操作手順を指示する方法<br>「コンテナを 1 つ起動しろ」のような操作 |
| 宣言的（Declarative） | 「どうあるべきか」の最終状態を宣言する方法<br>「コンテナが 3 つ動いている状態にしろ」のような宣言 |
| あるべき状態（Desired State） | システムが維持すべき目標の状態<br>管理者が宣言し、オーケストレーションが維持する |
| 調整ループ（Reconciliation Loop） | あるべき状態と実際の状態を継続的に比較し、差分を自動で修正するメカニズム |
| Borg | Google が社内で運用していた大規模クラスタ管理システム<br>Kubernetes の設計に影響を与えた |
| Kubernetes（K8s） | Google の Borg の経験を活かして設計された、オープンソースのコンテナオーケストレーション基盤<br>名前はギリシャ語で「操舵手」を意味する |
| CNCF（Cloud Native Computing Foundation） | Kubernetes を含むクラウドネイティブ技術のオープンソースプロジェクトを管理する中立的な組織 |
| レプリカ（Replica） | 同じコンテナの複製<br>負荷分散や可用性のために複数のレプリカを実行する |
| コントロールプレーン（Control Plane） | クラスタ全体を管理する中枢<br>スケジューリング、状態管理、API 提供などを担当する |

---

## [参考資料](#references) {#references}

このページの内容は、以下のソースに基づいています

<strong>学術論文</strong>

- "Large-scale cluster management at Google with Borg" (Verma et al., EuroSys 2015)
  - Google の大規模クラスタ管理システム Borg の設計と運用

- "Borg, Omega, and Kubernetes" (Burns et al., ACM Queue 2016)
  - Borg から Kubernetes への設計思想の変遷と教訓

<strong>Kubernetes</strong>

- [Kubernetes Documentation - Overview](https://kubernetes.io/docs/concepts/overview/){:target="\_blank"}
  - Kubernetes の概要と基本概念

- [Kubernetes Documentation - What is Kubernetes?](https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/){:target="\_blank"}
  - Kubernetes の定義、機能、歴史的背景

<strong>Compose</strong>

- [Compose Specification](https://docs.docker.com/compose/compose-file/){:target="\_blank"}
  - Docker Compose の仕様
